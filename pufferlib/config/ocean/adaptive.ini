[base]
package = ocean
env_name = puffer_adaptive_drive
policy_name = Drive
rnn_name = Recurrent

[vec]
num_workers = 16
num_envs = 16
batch_size = 1
; backend = Serial

[policy]
input_size = 64
hidden_size = 256

[rnn]
input_size = 256
hidden_size = 256

[env]
num_agents = 1024
; Options: discrete, continuous
action_type = discrete
; Options: classic, jerk
dynamics_model = classic
; Number of consecutive scenarios per episode (adaptive-specific)
k_scenarios = 2
reward_vehicle_collision = -0.5
reward_offroad_collision = -0.2
reward_ade = 0.0
dt = 0.1
reward_goal = 1.0
reward_goal_post_respawn = 0.25
; Meters around goal to be considered "reached"
goal_radius = 2.0
; What to do when the goal is reached. Options: 0:"respawn", 1:"generate_new_goals", 2:"stop"
goal_behavior = 0
; Options: 0 - Ignore, 1 - Stop, 2 - Remove
collision_behavior = 0
; Options: 0 - Ignore, 1 - Stop, 2 - Remove
offroad_behavior = 0
; Number of steps before reset
scenario_length = 91
; Resample frequency = k_scenarios * scenario_length (adaptive-specific)
resample_frequency = 182
num_maps = 1000
condition_type = "none" # Options: "none", "reward", "entropy", "discount", "all"
; Determines which step of the trajectory to initialize the agents at upon reset
init_steps = 0
; Options: "control_vehicles", "control_agents", "control_tracks_to_predict"
control_mode = "control_vehicles"
; Options: "created_all_valid", "create_only_controlled"
init_mode = "create_all_valid"


num_ego_agents = 512

; population play settings
population_play = True
co_player_policy_name = Drive
co_player_rnn_name = Recurrent
co_player_policy_path = "resources/drive/policies/varied_discount.pt"

; Conditioning settings for co-players
; Options: "none", "reward", "entropy", "discount", "all"
co_player_condition_type =  "all"
collision_weight_lb = -1.0
collision_weight_ub = 0.0
offroad_weight_lb = 0.0
offroad_weight_ub = -0.2
goal_weight_lb = 0.0
goal_weight_ub = 1.0
entropy_weight_lb = 0.0
entropy_weight_ub = 0.001
discount_weight_lb = 0.98
discount_weight_ub = 0.80


[env.co_player_policy]
input_size = 64
hidden_size = 256

[env.co_player_rnn]
input_size = 256
hidden_size = 256

[train]
total_timesteps = 2_000_000_000
; learning_rate = 0.02
; gamma = 0.985
anneal_lr = True
; Needs to be: num_agents * num_workers * BPTT horizon
batch_size = auto
minibatch_size = 372736
minibatch_multiplier = 512
max_minibatch_size = 372736
; BPTT horizon (overridden by pufferl.py for adaptive agents to k_scenarios * scenario_length)
bptt_horizon = 32
adam_beta1 = 0.9
adam_beta2 = 0.999
adam_eps = 1e-8
clip_coef = 0.2
ent_coef = 0.001
gae_lambda = 0.95
gamma = 0.98
learning_rate = 0.001
max_grad_norm = 1
prio_alpha = 0.8499999999999999
prio_beta0 = 0.8499999999999999
update_epochs = 1
vf_clip_coef = 0.1999999999999999
vf_coef = 2
vtrace_c_clip = 1
vtrace_rho_clip = 1
checkpoint_interval = 50
; Rendering options
render = True
render_interval = 50
; If True, show exactly what the agent sees in agent observation
obs_only = True
; Show grid lines
show_grid = False
; Draws lines from ego agent observed ORUs and road elements to show detection range
show_lasers = False
; Display human xy logs in the background
show_human_logs = True
; Options: str to path (e.g., "resources/drive/binaries/map_001.bin"), None
render_map = none

[wosac]
# WOSAC (Waymo Open Sim Agents Challenge) evaluation settings
backend = PufferEnv
enabled = False
num_rollouts = 32  # Number of policy rollouts per scene
init_steps = 10
num_total_wosac_agents = 256  # Total number of WOSAC agents to evaluate
init_mode = "create_all_valid"  # Initialize from the tracks to predict
control_mode = "control_tracks_to_predict"  # Control the tracks to predict
goal_behavior = 2  # Stop when reaching the goal (Note: needs to be fixed in configs)
sanity_check = False

[sweep.env.reward_vehicle_collision]
distribution = uniform
min = -0.5
max = 0.0
mean = -0.05
scale = auto

[sweep.env.reward_offroad_collision]
distribution = uniform
min = -0.5
max = 0.0
mean = -0.05
scale = auto

[sweep.env.goal_radius]
distribution = uniform
min = 2.0
max = 20.0
mean = 10.0
scale = auto

[sweep.env.reward_ade]
distribution = uniform
min = -0.1
max = 0.0
mean = -0.02
scale = auto

[sweep.env.reward_goal_post_respawn]
distribution = uniform
min = 0.0
max = 1.0
mean = 0.5
scale = auto
