[base]
package = ocean
env_name = puffer_drive
policy_name = Drive
rnn_name = Recurrent

[vec]
num_workers = 16
num_envs = 16
batch_size = 2
;backend = PufferEnv

[policy]
input_size = 64
hidden_size = 256

[rnn]
input_size = 256
hidden_size = 256

[env]
num_agents = 1024
action_type = "discrete" # discrete, continuous
use_goal_generation = False # True to generate new goals when agents reach their goal, False to respawn
reward_vehicle_collision = -0.5
reward_offroad_collision = -0.2
reward_ade = 0.0
reward_goal = 1.0
reward_goal_post_respawn = 0.25
goal_radius = 2.0 # Meters around goal to be considered "reached"
scenario_length = 91 # Number of steps to before reset
resample_frequency = 910
num_maps = 1000
init_steps = 10 # Determines which step of the trajectory to initialize the agents at upon reset
control_mode = "control_agents" # Options: "control_vehicles", "control_agents", "control_tracks_to_predict"
init_mode = "create_all_valid" # Options: "created_all_valid", "create_only_controlled"
max_controlled_agents = -1 # TODO: CURRENTLY NOT SUPPORTED.

[train]
total_timesteps = 3_000_000_000
#learning_rate = 0.02
#gamma = 0.985
anneal_lr = True
batch_size = 524288 # Needs to be: num_agents * num_workers * BPTT horizon
minibatch_size = 32768
max_minibatch_size = 32768
bptt_horizon = 32
adam_beta1 = 0.9
adam_beta2 = 0.999
adam_eps = 1e-8
clip_coef = 0.2
ent_coef = 0.005
gae_lambda = 0.95
gamma = 0.98
learning_rate = 0.001
max_grad_norm = 1
prio_alpha = 0.8499999999999999
prio_beta0 = 0.8499999999999999
update_epochs = 1
vf_clip_coef = 0.1999999999999999
vf_coef = 2
vtrace_c_clip = 1
vtrace_rho_clip = 1
checkpoint_interval = 500
# Rendering options
render = True
render_interval = 500
obs_only = True # If True, show exactly what the agent sees in agent observation
show_grid = False # Show grid lines
show_lasers = False # Draws lines from ego agent observed ORUs and road elements to show detection range
show_human_logs = True # Display human xy logs in the background
render_map = none  # Options: str to path (e.g., "resources/drive/binaries/map_001.bin"), None

[wosac]
# WOSAC (Waymo Open Sim Agents Challenge) evaluation settings
backend = PufferEnv
enabled = False
num_rollouts = 32  # Number of policy rollouts per scene
init_steps = 10
num_total_wosac_agents = 5  # Total number of WOSAC agents to evaluate
init_mode = "create_all_valid"  # Initialize from the tracks to predict
control_mode = "control_tracks_to_predict"  # Control the tracks to predict

[sweep.env.reward_vehicle_collision]
distribution = uniform
min = -0.5
max = 0.0
mean = -0.05
scale = auto

[sweep.env.reward_offroad_collision]
distribution = uniform
min = -0.5
max = 0.0
mean = -0.05
scale = auto

[sweep.env.goal_radius]
distribution = uniform
min = 2.0
max = 20.0
mean = 10.0
scale = auto

[sweep.env.reward_ade]
distribution = uniform
min = -0.1
max = 0.0
mean = -0.02
scale = auto

[sweep.env.reward_goal_post_respawn]
distribution = uniform
min = 0.0
max = 1.0
mean = 0.5
scale = auto
