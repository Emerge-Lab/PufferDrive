[base]
package = ocean
env_name = puffer_drive
policy_name = Drive
rnn_name = Recurrent

[vec]
num_workers = 16
num_envs = 16
batch_size = 4
#backend = Serial

[policy]
input_size = 64
hidden_size = 256

[rnn]
input_size = 256
hidden_size = 256

[env]
num_agents = 1024
; Options: discrete, continuous
action_type = discrete
; Options: classic, jerk
dynamics_model = classic
reward_vehicle_collision = -0.5
reward_offroad_collision = -0.2
reward_ade = 0.0
dt = 0.1
reward_goal = 1.0
reward_goal_post_respawn = 0.25
; Meters around goal to be considered "reached"
goal_radius = 2.0
; What to do when the goal is reached. Options: 0:"respawn", 1:"generate_new_goals", 2:"stop"
goal_behavior = 0
; Options: 0 - Ignore, 1 - Stop, 2 - Remove
collision_behavior = 0
; Options: 0 - Ignore, 1 - Stop, 2 - Remove
offroad_behavior = 0
; Number of steps before reset
scenario_length = 91
resample_frequency = 910
num_maps = 1000
; Determines which step of the trajectory to initialize the agents at upon reset
init_steps = 0
; Options: "control_vehicles", "control_agents", "control_tracks_to_predict", "control_sdc_only"
control_mode = "control_vehicles"
; Options: "created_all_valid", "create_only_controlled"
init_mode = "create_all_valid"

# Note: below do not propagate to C
road_points=128
other_objects=31

[train]
seed = 42
total_timesteps = 3_000_000_000
anneal_lr = True
; Needs to be: num_agents * num_workers * BPTT horizon
batch_size = 524288
minibatch_size = 32768
max_minibatch_size = 32768
bptt_horizon = 32
adam_beta1 = 0.9
adam_beta2 = 0.999
adam_eps = 1e-8
clip_coef = 0.2
ent_coef = 0.004670370966041502
gae_lambda = 0.95
gamma = 0.98
learning_rate = 0.003000000000000001
max_grad_norm = 1
prio_alpha = 0.8499999999999999
prio_beta0 = 0.8499999999999999
update_epochs = 1
vf_clip_coef = 0.1999999999999999
vf_coef = 2
vtrace_c_clip = 1
vtrace_rho_clip = 1

checkpoint_interval = 200
; Rendering options
render = False
render_interval = 200
; If True, show exactly what the agent sees in agent observation
obs_only = True
; Show grid lines
show_grid = False
; Draws lines from ego agent observed ORUs and road elements to show detection range
show_lasers = False
; Display human xy logs in the background
show_human_logs = True
; Options: str to path (e.g., "resources/drive/binaries/map_001.bin"), None
render_map = none

[eval]
eval_interval = 1000
backend = PufferEnv
; WOSAC (Waymo Open Sim Agents Challenge) evaluation settings
; If True, enables evaluation on realism metrics each time we save a checkpoint
wosac_realism_eval = False
wosac_num_rollouts = 32  # Number of policy rollouts per scene
wosac_init_steps = 10 # When to start the simulation
wosac_num_agents = 256  # Total number of WOSAC agents to evaluate
wosac_control_mode = "control_tracks_to_predict"  # Control the tracks to predict
wosac_init_mode = "create_all_valid"  # Initialize from the tracks to predict
wosac_goal_behavior = 2  # Stop when reaching the goal
wosac_goal_radius = 2.0 # Can shrink goal radius for WOSAC evaluation
wosac_sanity_check = False
wosac_aggregate_results = True # Only return aggregate results across all scenes
; If True, enable human replay evaluation (pair policy-controlled agent with human replays)
human_replay_eval = False
human_replay_control_mode = "control_sdc_only" # Control only the self-driving car
human_replay_num_agents = 64 # This equals the number of scenarios, since we control one agent in each

[sweep.train.learning_rate]
distribution = log_normal
min = 0.001
mean = 0.003
max = 0.01
scale = auto

[sweep.train.ent_coef]
distribution = log_normal
min = 0.001
mean = 0.005
max = 0.01
scale = auto

[sweep.train.gamma]
distribution = logit_normal
min = 0.97
mean = 0.98
max = 0.9999
scale = auto
