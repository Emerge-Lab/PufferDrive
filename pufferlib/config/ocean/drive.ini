[base]
package = ocean
env_name = puffer_drive
policy_name = Drive
rnn_name = Recurrent

[vec]
num_workers = 16
num_envs = 16
batch_size = 4
; backend = Serial

[policy]
input_size = 64
hidden_size = 256

[rnn]
input_size = 256
hidden_size = 256

[env]
num_agents = 1024
; Options: discrete, continuous
action_type = discrete
; Options: classic, jerk
dynamics_model = classic
reward_vehicle_collision = -0.5
reward_offroad_collision = -0.5
; Guided autonomy reward parameters
use_guided_autonomy = 0
guidance_speed_weight = 0.0
guidance_heading_weight = 0.0
waypoint_reach_threshold = 2.0
dt = 0.1
reward_goal = 1.0
reward_goal_post_respawn = 0.25
; Meters around goal to be considered "reached"
goal_radius = 2.0
; Max target speed in m/s for the agent to maintain towards the goal
goal_speed = 100.0
; What to do when the goal is reached. Options: 0:"respawn", 1:"generate_new_goals", 2:"stop"
goal_behavior = 0
; Determines the target distance to the new goal in the case of goal_behavior = generate_new_goals.
; Large numbers will select a goal point further away from the agent's current position.
goal_target_distance = 25.0
; Options: 0 - Ignore, 1 - Stop, 2 - Remove
collision_behavior = 0
; Options: 0 - Ignore, 1 - Stop, 2 - Remove
offroad_behavior = 0
; Number of steps before
episode_length = 91
resample_frequency = 910
termination_mode = 1 # 0 - terminate at episode_length, 1 - terminate after all agents have been reset
map_dir = "resources/drive/binaries/training"
num_maps = 10000
; Determines which step of the trajectory to initialize the agents at upon reset
init_steps = 0
; Options: "control_vehicles", "control_agents", "control_wosac", "control_sdc_only"
control_mode = "control_vehicles"
; Options: "created_all_valid", "create_only_controlled"
init_mode = "create_all_valid"

; Note: this argument is currently duplicated in [train] section and should match
bptt_horizon = 32
; Directory to save/load human demonstration data
human_data_dir = "pufferlib/resources/drive/human_demonstrations"
; Number of human samples to create per env. Each sequence has length equal to bptt_horizon.
; So with bptt_horizon=32, each sequence has 3 seconds of human data.
max_expert_sequences = 256

[train]
seed=42
total_timesteps = 2_000_000_000
; learning_rate = 0.02
; gamma = 0.985
anneal_lr = True
; Needs to be: num_agents * num_workers * BPTT horizon
batch_size = 524288
minibatch_size = 32768
max_minibatch_size = 32768
bptt_horizon = 32
adam_beta1 = 0.9
adam_beta2 = 0.999
adam_eps = 1e-8
clip_coef = 0.2
ent_coef = 0.005
gae_lambda = 0.95
gamma = 0.98
learning_rate = 0.003
max_grad_norm = 1
prio_alpha = 0.8499999999999999
prio_beta0 = 0.8499999999999999
update_epochs = 1
vf_clip_coef = 0.1999999999999999
vf_coef = 2
; How many randomly sampled human demonstration sequences to use per update
human_sequences = 64
human_clip_coef = 10.0
human_ll_coef = 0.0
vtrace_c_clip = 1
vtrace_rho_clip = 1
checkpoint_interval = 500
; Rendering options
render = False
render_interval = 500
; If True, show exactly what the agent sees in agent observation
obs_only = True
; Show grid lines
show_grid = True
; Draws lines from ego agent observed ORUs and road elements to show detection range
show_lasers = False
; Display human xy logs in the background
show_human_logs = False
; If True, zoom in on a part of the map. Otherwise, show full map
zoom_in = True
; Options: List[str to path], str to path (e.g., "resources/drive/training/binaries/map_001.bin"), None
render_map = none

[eval]
eval_interval = 1000
; Path to dataset used for evaluation
map_dir = "resources/drive/binaries/training"
; Evaluation will run on the first num_maps maps in the map_dir directory
num_maps = 20
backend = PufferEnv
; WOSAC (Waymo Open Sim Agents Challenge) evaluation settings
; If True, enables evaluation on realism metrics each time we save a checkpoint
wosac_realism_eval = False
; Number of policy rollouts per scene
wosac_num_rollouts = 32
; When to start the simulation
wosac_init_steps = 10
; Control everything valid at init in the scene
wosac_control_mode = "control_wosac"
; Create everything in valid at init the scene
wosac_init_mode = "create_all_valid"
; Stop when reaching the goal
wosac_goal_behavior = 2
; Can shrink goal radius for WOSAC evaluation
wosac_goal_radius = 2.0
wosac_sanity_check = False
; Only return aggregate results across all scenes
wosac_aggregate_results = True
; If True, enable human replay evaluation (pair policy-controlled agent with human replays)
human_replay_eval = False
; Control only the self-driving car
human_replay_control_mode = "control_sdc_only"

[sweep.train.learning_rate]
distribution = log_normal
min = 0.001
mean = 0.003
max = 0.005
scale = auto

[sweep.train.ent_coef]
distribution = log_normal
min = 0.001
mean = 0.005
max = 0.03
scale = auto

[sweep.train.gamma]
distribution = log_normal
min = 0.97
mean = 0.98
max = 0.999
scale = auto

[sweep]
method = ParetoGenetic
metric = score
goal = maximize
downsample = 10

; Commented out - enable if sweeping human demonstration parameters
; [sweep.train.human_ll_coef]
; distribution = log_normal
; min = 0.01
; max = 0.5
; mean = 0.02
; scale = auto

; [sweep.env.max_expert_sequences]
; distribution = log_normal
; min = 64
; max = 1024
; mean = 256
; scale = auto

; Guided autonomy reward parameter sweeps
[sweep.env.guidance_speed_weight]
distribution = uniform
min = 0.1
max = 0.5
mean = 0.3
scale = auto

[sweep.env.guidance_heading_weight]
distribution = uniform
min = 0.1
max = 0.5
mean = 0.3
scale = auto

[sweep.env.waypoint_reach_threshold]
distribution = uniform
min = 0.5
max = 3.0
mean = 1.5
scale = auto
