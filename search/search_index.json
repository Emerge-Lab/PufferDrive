{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"PufferDrive","text":"<p>PufferDrive is a high-throughput autonomous driving simulator built on PufferLib. Train and evaluate multi-agent driving policies with fast vectorized stepping, streamlined data conversion, and ready-made benchmarks.</p> Start here: install &amp; build See the workflow"},{"location":"#highlights","title":"Highlights","text":"<ul> <li>Multi-agent drive environment that trains agents at 300K steps per second.</li> <li>Scripts to convert Waymo Open Motion Dataset JSON into lightweight binaries (CARLA sample map included).</li> <li>Benchmarks for distributional realism and human compatibility.</li> <li>Raylib-based visualizer for local or headless render/export.</li> </ul>"},{"location":"#quick-start","title":"Quick start","text":"<ul> <li>Follow Getting Started to install, build the C extensions, and run <code>puffer train puffer_drive</code>.</li> <li>Consult Simulator for how actions/observations, rewards, and <code>.ini</code> settings map to the underlying C environment and Torch policy.</li> <li>Prepare drive map binaries with the steps in Data.</li> <li>Evaluate a policy with the commands in Evaluation and preview runs with the Visualizer.</li> </ul>"},{"location":"#workflow","title":"Workflow","text":"Step 1 Install &amp; Build <p>Set up the environment, install dependencies, and compile the native extensions.</p> Open guide Step 2 Prepare Data <p>Download WOMD/GPUDrive datasets from Hugging Face and convert to map binaries.</p> Open guide Step 3 Train &amp; Evaluate <p>Train agents and evaluate them with WOSAC and human-replay benchmarks.</p> Open guide"},{"location":"#repository-layout","title":"Repository layout","text":"<ul> <li><code>pufferlib/ocean/drive</code>: Drive environment implementation and map processing utilities.</li> <li><code>resources/drive/binaries</code>: Expected location for compiled map binaries (outputs of the data conversion step).</li> <li><code>scripts/build_ocean.sh</code>: Helper for building the Raylib visualizer and related binaries.</li> <li><code>examples</code>, <code>tests</code>, <code>experiments</code>: Reference usage, checks, and research scripts that pair with the docs pages.</li> </ul>"},{"location":"data/","title":"Data","text":"<p>PufferDrive consumes map binaries generated from multiple data sources, including the Waymo Open Motion Dataset (WOMD) JSON files, ScenarioMax, and CARLA. This page covers how to obtain data and convert it into the binary format expected by the simulator.</p>"},{"location":"data/#download-options","title":"Download options","text":"<ul> <li><code>pufferdrive_womd_train</code>: 10k scenarios from the Waymo Open Motion training dataset.</li> <li><code>pufferdrive_womd_val</code>: 10k scenarios from the Waymo Open Motion validation dataset.</li> <li>Additional compatible sources: ScenarioMax exports JSON in the same format.</li> <li>Included CARLA maps: Readily available CARLA maps live in <code>data_utils/carla/carla_py123d</code>.</li> </ul>"},{"location":"data/#download-via-hugging-face","title":"Download via Hugging Face","text":"<p>Install the CLI once:</p> <pre><code>uv pip install -U \"huggingface_hub[cli]\"\n</code></pre> <p>Download: <pre><code>huggingface-cli download daphne-cornelisse/pufferdrive_womd_train \\\n  --repo-type dataset \\\n  --local-dir data/processed/training\n</code></pre></p> <p>Place raw JSON files under <code>data/processed/training</code> (default location read by the conversion script).</p>"},{"location":"data/#convert-json-to-map-binaries","title":"Convert JSON to map binaries","text":"<p>The conversion script writes compact <code>.bin</code> maps to <code>resources/drive/binaries</code>:</p> <pre><code>python pufferlib/ocean/drive/drive.py\n</code></pre> <p>Notes:</p> <ul> <li>The script iterates every JSON file in <code>data/processed/training</code> and emits <code>map_XXX.bin</code> files.</li> <li><code>resources/drive/binaries/map_000.bin</code> ships with the repo for quick smoke tests; generate additional bins for training/eval.</li> <li>If you want to point at a different dataset location or limit the number of maps, adjust <code>process_all_maps</code> in <code>pufferlib/ocean/drive/drive.py</code> before running.</li> </ul>"},{"location":"data/#map-binary-format-reference","title":"Map binary format reference","text":"<p>The simulator reads the compact binary layout produced by <code>save_map_binary</code> in <code>pufferlib/ocean/drive/drive.py</code> and parsed by <code>load_map_binary</code> in <code>pufferlib/ocean/drive/drive.h</code>:</p> <ul> <li>Header: <code>sdc_track_index</code> (int), <code>num_tracks_to_predict</code> (int) followed by that many <code>track_index</code> ints, <code>num_objects</code> (int), <code>num_roads</code> (int).</li> <li>Objects (vehicles/pedestrians/cyclists): For each object, the writer stores <code>scenario_id</code> (<code>unique_map_id</code> passed to <code>load_map</code>), <code>type</code> (<code>1</code> vehicle, <code>2</code> pedestrian, <code>3</code> cyclist), <code>id</code>, <code>array_size</code> (<code>TRAJECTORY_LENGTH = 91</code>), positions <code>x/y/z[91]</code>, velocities <code>vx/vy/vz[91]</code>, <code>heading[91]</code>, <code>valid[91]</code>, and scalars <code>width/length/height</code>, <code>goalPosition (x, y, z)</code>, <code>mark_as_expert</code> (int). Missing trajectory entries are zero-padded by the converter.</li> <li>Road elements: Each road entry stores <code>scenario_id</code>, a remapped <code>type</code> (<code>4</code> lane, <code>5</code> road line, <code>6</code> road edge, <code>7</code> stop sign, <code>8</code> crosswalk, <code>9</code> speed bump, <code>10</code> driveway), <code>id</code>, <code>array_size</code> (#points), then <code>x/y/z</code> arrays of that length and scalars <code>width/length/height</code>, <code>goalPosition</code>, <code>mark_as_expert</code>. <code>save_map_binary</code> also simplifies long polylines (<code>len(geometry) &gt; 10</code> and <code>type &lt;= 16</code>) with a 0.1 area threshold to keep files small.</li> <li>Control hints: <code>tracks_to_predict</code> and <code>mark_as_expert</code> influence which agents are controllable (<code>control_mode</code> in the simulator) versus replayed as experts or static actors (<code>set_active_agents</code> in <code>drive.h</code>).</li> </ul> <p>Refer to Simulator for how the binaries are consumed during resets, observation construction, and reward logging.</p>"},{"location":"data/#verifying-data-availability","title":"Verifying data availability","text":"<ul> <li>After conversion, <code>ls resources/drive/binaries | head</code> should show numbered <code>.bin</code> files.</li> <li>If you see <code>Required directory resources/drive/binaries/map_000.bin not found</code> during training, rerun the conversion or check paths.</li> <li>With binaries in place, run <code>puffer train puffer_drive</code> from Getting Started as a smoke test that the build, data, and bindings are wired together.</li> <li>To inspect the binary output, convert a single JSON file with <code>load_map(&lt;json&gt;, &lt;id&gt;, &lt;output_path&gt;)</code> inside <code>drive.py</code>.</li> </ul>"},{"location":"data/#interactive-scenario-editor","title":"Interactive scenario editor","text":"<p>See Interactive scenario editor for a browser-based workflow to inspect, edit, and export Waymo/ScenarioMax JSON into the <code>.bin</code> format consumed by the simulator.</p>"},{"location":"data/#generate-carla-agent-trajectories","title":"Generate CARLA agent trajectories","text":"<p>The agent trajectories in the provided CARLA maps are procedurally generated assuming a general velocity range without a valid initial state(no collision/offroad). The repository uses an external submodule for CARLA XODR processing (<code>pyxodr</code>).</p> <p>To generate your own CARLA agent trajectories, install the submodules and developer requirements (editable install) before running the generator:</p> <pre><code>git submodule update --init --recursive\n\npython -m pip install -e . -r requirements-dev.txt\n</code></pre> <p>Run the generator script. Important optional args:</p> <ul> <li><code>--num_objects</code>: how many agents to initialize in a map (default: map-dependent)</li> <li><code>--num_data_per_map</code>: number of data files to generate per map</li> <li><code>--avg_speed</code>: controls the gap between subsequent points in the trajectory</li> </ul> <pre><code>python data_utils/carla/generate_carla_agents.py --num_objects 32 --num_data_per_map 8 --avg_speed 2\n</code></pre> <p>There is also a visualizer for inspecting initial agent positions on the map:</p> <pre><code>python data_utils/carla/plot.py\n</code></pre> <p>Notes:</p> <ul> <li>Base Carla maps that agents are spawned live under <code>data_utils/carla/carla_py123d</code> and the Carla XODRs are at <code>data/CarlaXODRs</code> to interact with the <code>pyxodr</code> submodule for XODR parsing and agent traj generation.</li> <li>If you encounter missing binary or map errors, ensure the submodule was initialized and the required packages from <code>requirements-dev.txt</code> are installed.</li> </ul>"},{"location":"evaluation/","title":"Evaluations and benchmarks","text":"<p>Driving is a safety-critical multi-agent application, making careful evaluation and risk assessment essential. Mistakes in the real world are costly, so simulations are used to catch errors before deployment. To support rapid iteration, evaluations should ideally run efficiently. This is why we also paid attention to optimizing the speed of the evaluations. This page contains an overview of the available benchmarks and evals.</p>"},{"location":"evaluation/#sanity-maps","title":"Sanity maps \ud83d\udc1b","text":"<p>Quickly test the training on curated, lightweight scenarios without downloading the full dataset. Each sanity map tests a specific behavior.</p> <pre><code>puffer sanity puffer_drive --wandb --wandb-name sanity-demo --sanity-maps forward_goal_in_front s_curve\n</code></pre> <p>Or run them all at once:</p> <pre><code>puffer sanity puffer_drive --wandb --wandb-name sanity-all\n</code></pre> <ul> <li>Tip: turn learning-rate annealing off for these short runs (<code>--train.anneal_lr False</code>) to keep the sanity checks from decaying the optimizer mid-run.</li> </ul> <p>Available maps:</p> <ul> <li><code>forward_goal_in_front</code>: Straight approach to a goal in view.</li> <li><code>reverse_goal_behind</code>: Backward start with a behind-the-ego goal.</li> <li><code>two_agent_forward_goal_in_front</code>: Two agents advancing to forward goals.</li> <li><code>two_agent_reverse_goal_behind</code>: Two agents reversing to rear goals.</li> <li><code>simple_turn</code>: Single, gentle turn to a nearby goal.</li> <li><code>s_curve</code>: S-shaped path with alternating curvature.</li> <li><code>u_turn</code>: U-shaped turn to a goal behind the start.</li> <li><code>one_or_two_point_turn</code>: Tight turn requiring a small reversal.</li> <li><code>three_or_four_point_turn</code>: Even tighter turn needing multiple reversals.</li> <li><code>goal_out_of_sight</code>: Goal starts without direct path; needs some planning.</li> </ul> <p></p>"},{"location":"evaluation/#distributional-realism-benchmark","title":"Distributional realism benchmark \ud83d\udcca","text":"<p>We provide a PufferDrive implementation of the Waymo Open Sim Agents Challenge (WOSAC) for fast, easy evaluation of how well your trained agent matches distributional properties of human behavior.</p> <pre><code>puffer eval puffer_drive --eval.wosac-realism-eval True\n</code></pre> <p>Add <code>--load-model-path &lt;path_to_checkpoint&gt;.pt</code> to score a trained policy, instead of a random baseline.</p> <p>See the WOSAC benchmark page for the metric pipeline and all the details.</p>"},{"location":"evaluation/#human-compatibility-benchmark","title":"Human-compatibility benchmark \ud83e\udd1d","text":"<p>You may be interested in how compatible your agent is with human partners. For this purpose, we support an eval where your policy only controls the self-driving car (SDC). The rest of the agents in the scene are stepped using the logs. While it is not a perfect eval since the human partners here are static, it will still give you a sense of how closely aligned your agent's behavior is to how people drive. You can run it like this:</p> <pre><code>puffer eval puffer_drive --eval.human-replay-eval True --load-model-path &lt;path_to_checkpoint&gt;.pt\n</code></pre> <p>During this evaluation the self-driving car (SDC) is controlled by your policy while other agents replay log trajectories.</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This page walks through installing PufferDrive from source, building the native extensions, and running a first training job.</p>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.9+ with a virtual environment manager (<code>uv</code>, <code>venv</code>, or <code>conda</code>).</li> <li>A C/C++ toolchain for building the bundled extensions (GCC/Clang + make).</li> <li>PyTorch installed inside your environment (pick the CPU/GPU wheel that matches your setup).</li> </ul>"},{"location":"getting-started/#installation","title":"Installation","text":"<p>Clone and set up an isolated environment:</p> <pre><code>git clone https://github.com/Emerge-Lab/PufferDrive.git\ncd PufferDrive\nuv venv .venv &amp;&amp; source .venv/bin/activate\nuv pip install -e .\n</code></pre> <p>Build the C extensions in place:</p> <p><pre><code>python setup.py build_ext --inplace --force\n</code></pre> Run this with your virtual environment activated so the compiled extension links against the correct Python.</p>"},{"location":"getting-started/#when-to-rebuild-the-extension","title":"When to rebuild the extension","text":"<ul> <li>Re-run <code>python setup.py build_ext --inplace --force</code> after changing any C/Raylib sources in <code>pufferlib/ocean/drive</code> (e.g., <code>drive.c</code>, <code>drive.h</code>, <code>binding.c</code>, <code>visualize.c</code>) or after pulling upstream changes that touch those files. This regenerates the <code>binding.cpython-*.so</code> used by <code>Drive</code>.</li> <li>Pure Python edits (training scripts, docs, data utilities) do not require a rebuild; just restart your Python process.</li> </ul>"},{"location":"getting-started/#verify-the-setup","title":"Verify the setup","text":"<p>Once map binaries are available (see Data), launch a quick training run to confirm the environment, data, and bindings are wired up correctly:</p> <pre><code>puffer train puffer_drive\n</code></pre> <p>For multi-node training(Only uses Data Parallelism with torch ddp) <pre><code>torchrun --standalone --nnodes=1 --nproc-per-node=6 -m puffer train puffer_drive\n</code></pre></p> <p>If map binaries are missing, follow the steps in Data to generate them before training. See Visualizer for rendering runs and Evaluation for benchmark commands.</p>"},{"location":"getting-started/#logging-with-weights-biases","title":"Logging with Weights &amp; Biases","text":"<p>Enable W&amp;B logging with the built-in CLI flags (the package is already a dependency in <code>setup.py</code>):</p> <pre><code>puffer train puffer_drive --wandb --wandb-project pufferdrive --wandb-group local-dev\n</code></pre> <ul> <li>Add <code>--wandb</code> to turn on logging; <code>--wandb-project</code> and <code>--wandb-group</code> set the destination in W&amp;B.</li> <li>Checkpoint uploads and evaluation helpers (<code>pufferlib/utils.py</code>) will log WOSAC/human-replay metrics and rendered videos when W&amp;B is enabled.</li> </ul>"},{"location":"pufferdrive-2.0/","title":"PufferDrive 2.0: A fast and friendly driving simulator for training and evaluating RL agents","text":"<p>Daphne Cornelisse\u00b9\u00b7, Spencer Cheng\u00b2\u00b7, Pragnay Mandavilli\u00b9, Julian Hunt\u00b9, Kevin Joseph\u00b9, Wa\u00ebl Doulazmi\u00b3, Eugene Vinitsky\u00b9</p> <p>\u00b9Emerge Lab at NYU | \u00b2Puffer.ai | \u00b3Valeo | *Shared first authorship</p> <p>December 12, 2025</p> <p>We introduce PufferDrive 2.0, a fast and easy-to-use driving simulator for reinforcement learning. It supports training at up to 300,000 steps per second on a single GPU, enabling agents to reach strong performance in just a few hours. Evaluation and visualization run directly in the browser.</p> <p>This post outlines the design goals, highlights the main features, and shows what works out of the box. We conclude with a brief roadmap.</p>"},{"location":"pufferdrive-2.0/#introduction-and-history","title":"Introduction and history","text":"<p>Deep reinforcement learning algorithms, such as PPO, are highly effective in the billion-sample regime. A consistent finding across domains is that, given sufficient environmental signal and enough data, any precisely specified objective can, in principle, be optimized.</p> <p>This shifts the primary bottleneck to simulation. The faster we can generate high-quality experience, the more reliably we can apply RL to hard real-world problems, such as autonomous navigation in dynamic, unstructured environments.<sup>1</sup></p> <p>Over the past few years, several simulators have demonstrated that large-scale self-play can work for driving. Below, we summarize this progression and explain how it led to PufferDrive 2.0.</p>"},{"location":"pufferdrive-2.0/#early-results-with-self-play-rl-in-autonomous-driving","title":"Early results with self-play RL in autonomous driving","text":"<p>Nocturne was the first paper to show that self-play RL could work for driving at scale. Using maps from the Waymo Open Motion Dataset (WOMD), PPO agents achieved around an 80% goal-reaching rate without any human data.</p> <p>The main limitation was speed. Nocturne ran at roughly 2,000 steps per second, leading to multi-day training times and a complex setup process.</p> <p>The results were promising, but it was clear that scale was a major constraint.</p>"},{"location":"pufferdrive-2.0/#scaling-up","title":"Scaling up","text":"<p>Subsequent work showed what becomes possible when scale is no longer the bottleneck.</p> <ul> <li>Gigaflow demonstrated that large-scale self-play alone can produce robust, naturalistic driving. Using a highly batched simulator, it trained on the equivalent of decades of driving experience per hour and achieved state-of-the-art performance across multiple autonomous driving benchmarks\u2014without using any human data.</li> <li>GPUDrive, built on Madrona, showed that similar behavior could be learned in about one day on a single consumer GPU, using a simple reward function and a standard PPO implementation.</li> </ul> <p>These empirical results support the hypothesis that robust autonomous driving policies can be trained in the billion-sample regime without any human data.</p> <p> Figure 1: Progression of RL-based driving simulators. Left: end-to-end training throughput on an NVIDIA RTX 4080, counting only transitions collected by learning policy agents (excluding padding agents). Right: wall-clock time (log scale) required to reach an 80% goal-reaching rate. This metric captures both simulation speed and algorithmic efficiency.</p>"},{"location":"pufferdrive-2.0/#from-gpudrive-to-pufferdrive","title":"From GPUDrive to PufferDrive","text":"<p>While GPUDrive delivered impressive raw simulation speed, end-to-end training throughput of around 50K steps per second remained a limiting factor. This was particularly true on large maps such as CARLA. Memory layout and batching overheads, rather than simulation fidelity, became the dominant constraints.</p> <p>Faster end-to-end training is critical because it enables tighter debugging loops, broader experimentation, and faster scientific and engineering progress. This led directly to the development of PufferDrive.</p> <p>We partnered with Spencer Cheng from Puffer.ai to rebuild the system around the principles of PufferLib. Spencer reimplemented GPUDrive. The result was PufferDrive 1.0, reaching approximately 200,000 steps per second on a single GPU and scaling linearly across multiple GPUs. Training agents to solve 10,000 maps from the Waymo datset took roughly 24 hours with GPUDrive. With PufferDrive, the same results could now be reproduced in about 2 hours.</p>"},{"location":"pufferdrive-2.0/#pufferdrive-20","title":"PufferDrive 2.0","text":"<p>PufferDrive 2.0 builds on this foundation and [TODO]:</p> <ul> <li>Built-in evaluations, including a standard benchmark</li> <li>Support for multiple real-world datasets (WOMD, Carla)</li> <li>Speed improvement (200K -&gt; 300K)</li> <li>Extended browser-based visualization and analysis tools</li> </ul> <p>To our knowledge, PufferDrive 2.0 is among the fastest open-source driving simulators available today, while remaining accessible to new users.</p>"},{"location":"pufferdrive-2.0/#highlights","title":"Highlights","text":"<p>TODO</p>"},{"location":"pufferdrive-2.0/#roadmap","title":"Roadmap","text":"<p>TODO</p>"},{"location":"pufferdrive-2.0/#citation","title":"Citation","text":"<p>If you use PufferDrive in your research, please cite: <pre><code>@software{pufferdrive2024github,\n  author = {Daphne Cornelisse* and Spencer Cheng* and Pragnay Mandavilli and Julian Hunt and Kevin Joseph and Wa\u00ebl Doulazmi and Eugene Vinitsky},\n  title = {{PufferDrive}: A Fast and Friendly Driving Simulator for Training and Evaluating {RL} Agents},\n  url = {https://github.com/Emerge-Lab/PufferDrive},\n  version = {2.0.0},\n  year = {2025},\n}\n</code></pre> *Equal contribution</p> <ol> <li> <p>A useful parallel comes from the early days of computing. In the 1970s and 1980s, advances in semiconductor manufacturing and microprocessor design\u2014such as Intel\u2019s 8080 and 80286 chips\u2014dramatically reduced computation costs and increased speed. This made iterative software development accessible and enabled entirely new ecosystems of applications, ultimately giving rise to the personal computer. Multi-agent RL faces a similar bottleneck today: progress is limited by the cost and speed of experience collection. Fast, affordable simulation with integrated RL algorithms may play a similar catalytic role, enabling solutions that were previously out of reach.\u00a0\u21a9</p> </li> </ol>"},{"location":"scene-editor/","title":"Interactive scenario editor","text":"<p>A browser-based playground for inspecting and editing Waymo Open Motion Dataset (WOMD) scenes. The tool runs fully client-side at https://womd-editor.vercel.app/ and works directly with the JSON format produced by Waymo/ScenarioMax exports and PufferDrive conversions.</p>"},{"location":"scene-editor/#video-walkthrough","title":"Video walkthrough","text":""},{"location":"scene-editor/#quick-start","title":"Quick start","text":"<ul> <li>Open https://womd-editor.vercel.app/ in a modern Chromium/Firefox browser.</li> <li>Click Import JSON\u2026 in the left sidebar and drop one or more scenario files (Waymo/ScenarioMax JSON or editor exports).</li> <li>The app stores everything in-memory only; nothing is uploaded to a server.</li> </ul>"},{"location":"scene-editor/#what-you-can-do","title":"What you can do","text":"<ul> <li>Inspect: Top-down canvas with zoom/pan/rotate, agent labels, and a playback timeline with variable speed.</li> <li>Edit trajectories: Select an agent and tweak paths via drag handles, draw a polyline with the Line tool, freehand record a path, or drive the agent with keyboard controls (WASD/arrow keys, Space to brake, Enter to save, Esc to cancel).</li> <li>Edit roads: Switch to Road mode to draw or refine lane/edge/crosswalk geometry, recolor vertices by elevation, and view the lane connectivity overlay when ROAD_LANE/ROAD_LINE data exists.</li> <li>Configure metadata: Rename the scenario, toggle label mode (ID vs. array index), mark agents as experts, and choose which agents belong to <code>tracks_to_predict</code>.</li> <li>Export: Preview changes versus the import baseline, then download either Waymo-style JSON or a compact <code>.bin</code> suitable for PufferDrive\u2019s loader.</li> </ul>"},{"location":"scene-editor/#editing-workflow","title":"Editing workflow","text":"<ol> <li>Load a scene: Import one or multiple JSONs; each appears as a row in the Scenarios list with a quick delete button.</li> <li>Playback: Use the timeline to scrub frames or Space/Arrow keys to play/pause/step. Agent labels and trajectory visibility can be toggled in the editor panel.</li> <li>Trajectory tools (Trajectory mode):</li> <li>Adjust Path: Drag existing vertices/handles on the canvas.</li> <li>Line Tool: Click to lay out a polyline, set per-segment duration (seconds), then Apply Path to rebuild timestamps/velocity.</li> <li>Record Path: Freehand capture a path with the pointer; playback resets to frame 0.</li> <li>Drive Agent: Enter a lightweight driving loop; W/A/S/D or arrow keys steer, Space brakes, Enter saves, Esc cancels. Tunable speed/accel/steer sliders live under \u201cDrive Tune.\u201d</li> <li>Road tools (Road mode):</li> <li>Edit Geometry: Select segments/vertices to move, insert, split, or delete (Shift/Ctrl-click to insert on-canvas; Alt/Cmd-click to delete).</li> <li>Draw Road: Click to add vertices; Enter finishes, Esc cancels. Set the default Z used for new vertices in the right-hand panel.</li> <li>Type &amp; overlays: Tag segments as ROAD_LANE / ROAD_EDGE / ROAD_LINE / CROSSWALK / OTHER. Enable Color by Z to visualize elevation and Lane Graph to see lane entry/exit nodes plus downstream arrows.</li> <li>Export &amp; diff: Hit Export to open a preview modal that summarizes changes (metadata, agents, roads, tracks_to_predict, bounds, frames). Download JSON for round-tripping or <code>.bin</code> for simulator ingestion.</li> </ol>"},{"location":"scene-editor/#using-exports-with-pufferdrive","title":"Using exports with PufferDrive","text":"<ul> <li>JSON exports retain the Waymo layout (<code>objects</code>, <code>roads</code>, <code>tracks_to_predict</code>, <code>tl_states</code>, <code>metadata</code>) and can be converted or re-imported.</li> <li><code>.bin</code> exports match the compact format read by <code>pufferlib/ocean/drive/drive.py</code>; drop them into <code>resources/drive/binaries</code> (e.g., <code>map_000.bin</code>) to test inside the simulator.</li> <li>The editor auto-fills missing headings/speeds and clamps degenerate lanes to keep bounds reasonable; always spot-check via the Export preview before committing.</li> </ul>"},{"location":"scene-editor/#notes","title":"Notes","text":"<ul> <li>The app is currently work-in-progress; there is no persistent storage or backend sync.</li> <li>Large scenes may render slowly on low-power GPUs\u2014hide trajectories or road overlays to keep the canvas responsive.</li> <li>Source lives in the <code>WOMD-Editor/web</code> directory of this repo if you want to run it locally with <code>npm install &amp;&amp; npm run dev</code>.</li> </ul>"},{"location":"simulator/","title":"Simulator Guide","text":"<p>Deep dive into how the Drive environment is wired, what it expects as inputs, and how observations/actions/configs are shaped. The environment entrypoint is <code>pufferlib/ocean/drive/drive.py</code>, which wraps the C core in <code>pufferlib/ocean/drive/drive.h</code> via <code>binding.c</code>.</p>"},{"location":"simulator/#runtime-inputs-and-lifecycle","title":"Runtime inputs and lifecycle","text":"<ul> <li>Map binaries: The environment scans <code>resources/drive/binaries</code> for <code>map_*.bin</code> files and requires at least one to load. Keep <code>num_maps</code> no larger than what is present on disk. During vectorized setup, <code>binding.shared</code> samples maps until it accumulates at least <code>num_agents</code> controllable entities, skipping maps with no valid agents (<code>set_active_agents</code> in <code>drive.h</code>).</li> <li>Episode length: Default <code>episode_length = 91</code> to match the Waymo logs (trajectory data is 91 steps), but you can set <code>env.episode_length</code> (CLI or <code>.ini</code>) to any positive value. Metrics are logged and <code>c_reset</code> is called when <code>timestep = episode_length</code>.</li> <li>Resampling maps: Python-side <code>Drive.step</code> reinitializes the vectorized environments every <code>resample_frequency</code> steps (default <code>910</code>, ~10 episodes) with fresh map IDs and seeds.</li> <li>Initialization controls:</li> <li><code>init_steps</code> starts agents from a later timestep in the logged trajectory.</li> <li><code>init_mode</code> (<code>create_all_valid</code> vs <code>create_only_controlled</code>) decides which logged actors are instantiated at reset.</li> <li><code>control_mode</code> (<code>control_vehicles</code>, <code>control_agents</code>, <code>control_tracks_to_predict</code>, <code>control_sdc_only</code>) selects which instantiated actors are policy-controlled. Non-controlled actors can still appear as static or expert replay agents.</li> <li><code>goal_behavior</code> chooses what happens on goal reach (<code>0</code> respawn at start pose, <code>1</code> sample new lane-following goals via the lane topology graph, <code>2</code> stop in place). <code>goal_radius</code> sets the completion threshold in meters.</li> </ul> <p>See Data for how to produce the <code>.bin</code> inputs, including the binary layout.</p>"},{"location":"simulator/#actions-and-dynamics","title":"Actions and dynamics","text":"<ul> <li>Action types (<code>env.action_type</code>):</li> <li><code>discrete</code> (default): classic dynamics use a single <code>MultiDiscrete([7*13])</code> index decoded into acceleration (<code>ACCELERATION_VALUES</code>) and steering (<code>STEERING_VALUES</code>); jerk dynamics use <code>MultiDiscrete([4, 3])</code> over <code>JERK_LONG</code>/<code>JERK_LAT</code>.</li> <li><code>continuous</code>: a 2-D Box in <code>[-1, 1]</code>. Classic scales to the max accel/steer magnitudes used in the discrete table. Jerk scales asymmetrically: negative values reach up to <code>-15 m/s^3</code> braking, positives up to <code>4 m/s^3</code> acceleration, lateral jerk up to <code>\u00b14 m/s^3</code>.</li> <li>Dynamics models (<code>env.dynamics_model</code>):</li> <li><code>classic</code>: bicycle model integrating accel/steer with <code>dt</code> (default <code>0.1</code>).</li> <li><code>jerk</code>: integrates longitudinal/lateral jerk into accel, then into velocity/pose with steering limited to <code>\u00b10.55 rad</code>. Speeds are clipped to <code>[0, 20] m/s</code>.</li> </ul>"},{"location":"simulator/#observation-space","title":"Observation space","text":"<p>Shape is <code>ego_features + 63 * 7 + 200 * 7</code> = <code>1848</code> for classic dynamics (<code>ego_features = 7</code>) or <code>1851</code> for jerk dynamics (<code>ego_features = 10</code>). Computed in <code>compute_observations</code> (<code>drive.h</code>):</p> <ul> <li>Ego block (classic):</li> <li>Goal position in ego frame (x, y) scaled by <code>0.005</code> (~200 m range to 1.0)</li> <li>Ego speed / <code>MAX_SPEED</code> (100 m/s)</li> <li>Width / <code>MAX_VEH_WIDTH</code> (15 m)</li> <li>Length / <code>MAX_VEH_LEN</code> (30 m)</li> <li>Collision flag (1 if the agent collided this step)</li> <li>Respawn flag (1 if the agent was respawned this episode)</li> <li>Ego block additions (jerk dynamics model only):</li> <li>Steering angle / \u03c0</li> <li>Longitudinal acceleration normalized to <code>[-15, 4]</code></li> <li>Lateral acceleration normalized to <code>[-4, 4]</code></li> <li>Respawn flag (index 9)</li> <li>Partner blocks: Up to <code>MAX_AGENTS-1</code> other agents (active first, then static experts) within 50 m. Each uses 7 values: relative (x, y) in ego frame scaled by  <code>0.02</code>, width/length normalized as above, relative heading encoded as <code>(cos \u0394\u03b8, sin \u0394\u03b8)</code>, and speed / <code>MAX_SPEED</code>. Zero-padded when fewer neighbors are present or when agents are in respawn.</li> <li>Road blocks: Up to 200 nearby road segments pulled from a precomputed grid (<code>vision_range = 21</code>). Each entry stores relative midpoint (x, y) scaled by <code>0.02</code>, segment length / <code>MAX_ROAD_SEGMENT_LENGTH</code> (100 m), width / <code>MAX_ROAD_SCALE</code> (100), <code>(cos, sin)</code> of the segment direction in ego frame, and a type ID (<code>ROAD_LANE</code>..<code>DRIVEWAY</code> stored as <code>0..6</code>). Remaining slots are zero-padded.</li> </ul>"},{"location":"simulator/#rewards-termination-and-metrics","title":"Rewards, termination, and metrics","text":"<ul> <li>Per-step rewards (<code>c_step</code>):</li> <li>Collision with another actor: <code>reward_vehicle_collision</code> (default <code>-0.5</code>)</li> <li>Off-road (road-edge intersection): <code>reward_offroad_collision</code> (default <code>-0.2</code>)</li> <li> <p>Goal reached: <code>reward_goal</code> (default <code>1.0</code>) or <code>reward_goal_post_respawn</code> after a respawn</p> </li> <li> <p>Termination: No early truncation; episodes roll to episode_length steps. If <code>goal_behavior</code> is respawn, <code>respawn_agent</code> resets the pose and marks <code>respawn_timestep</code> so the respawn flag shows up in observations.</p> </li> <li>Logged metrics (<code>add_log</code> aggregates over all active agents across envs):</li> <li><code>score</code>: reached goal without collision/off-road</li> <li><code>collision_rate</code> / <code>offroad_rate</code>: fraction of agents with \u22651 event in the episode</li> <li><code>avg_collisions_per_agent</code> / <code>avg_offroad_per_agent</code>: counts per agent, capturing repeated events</li> <li><code>completion_rate</code>: reached goal (even if collided/off-road); <code>dnf_rate</code>: clean but never reached goal</li> <li><code>lane_alignment_rate</code>, <code>avg_displacement_error</code>, <code>num_goals_reached</code>, plus counts of active/static/expert agents</li> </ul> <p><code>collision_behavior</code>, <code>offroad_behavior</code>, <code>reward_vehicle_collision_post_respawn</code>, and <code>spawn_immunity_timer</code> are parsed from the INI but currently unused in the stepping logic.</p>"},{"location":"simulator/#configuration-files-ini","title":"Configuration files (<code>.ini</code>)","text":"<p><code>pufferlib/config/default.ini</code> supplies global defaults. Environment-specific overrides live in <code>pufferlib/config/ocean/drive.ini</code> and are loaded first when you run <code>puffer train puffer_drive</code>; CLI flags (e.g., <code>--env.num-maps 128</code>) override both.</p> <p>Key sections in <code>pufferlib/config/ocean/drive.ini</code>:</p> <ul> <li>[env]: Simulator knobs: <code>num_agents</code> (policy slots, C core cap 64), <code>num_maps</code>, episode_length, <code>resample_frequency</code>, <code>action_type</code>, <code>dynamics_model</code>, rewards, <code>goal_radius</code>, <code>goal_behavior</code>, <code>init_steps</code>, <code>init_mode</code>, <code>control_mode</code>; rendering toggles <code>render</code>, <code>render_interval</code>, <code>obs_only</code>, <code>show_grid</code>, <code>show_lasers</code>, <code>show_human_logs</code>, <code>render_map</code>.</li> <li>[vec]: Vectorization sizing (<code>num_envs</code>, <code>num_workers</code>, <code>batch_size</code>; backend defaults to multiprocessing).</li> <li>[policy]/[rnn]: Model widths for the Torch policy (<code>input_size</code>, <code>hidden_size</code>) and optional LSTM wrapper.</li> <li>[train]: PPO-style hyperparameters (timesteps, learning rate, clipping, batch/minibatch, BPTT horizon, optimizer choice) merged with any unspecified defaults from <code>pufferlib/config/default.ini</code>.</li> <li>[eval]: WOSAC/human-replay switches and sizing (<code>eval.wosac_*</code>, <code>eval.human_replay_*</code>) mapped directly to the <code>Drive</code> kwargs in evaluation subprocesses.</li> </ul>"},{"location":"simulator/#model-overview","title":"Model overview","text":"<p>Defined in <code>pufferlib/ocean/torch.py:Drive</code>:</p> <ul> <li>Three MLP encoders (ego, partners, roads) with LayerNorm. Partner and road encodings are max-pooled across instances.</li> <li>Concatenated embedding \u2192 GELU \u2192 linear to <code>hidden_size</code>, then split into actor/value heads.</li> <li>Discrete actions are emitted as logits per dimension (<code>MultiDiscrete</code>), continuous actions as Gaussian parameters (<code>softplus</code> std). Value head is a single linear output.</li> <li><code>Recurrent = pufferlib.models.LSTMWrapper</code> can wrap the policy using the <code>rnn</code> config entries; otherwise the policy is feed-forward.</li> </ul>"},{"location":"simulator/#drive-source-files-what-lives-where","title":"Drive source files (what lives where)","text":"<ul> <li><code>pufferlib/ocean/drive/drive.py</code>: Python Gymnasium-style wrapper that sets up buffers, validates map availability, seeds the C core via <code>binding.env_init</code>, and handles map resampling.</li> <li><code>pufferlib/ocean/drive/drive.h</code>: Main C implementation of stepping, observations, rewards/metrics, grid map, lane graph, and collision checking.</li> <li><code>pufferlib/ocean/drive/binding.c</code>: Python C-extension glue that exposes <code>Drive</code> to Python, handles shared buffer setup, logging, and reading the <code>.ini</code> config.</li> <li><code>pufferlib/ocean/drive/visualize.c</code>: Raylib-based renderer used by the <code>visualize</code> binary and training video exports.</li> <li><code>pufferlib/ocean/drive/drive.c</code>: Small C demo/perf harness and network parity test runner for the C policy head.</li> <li><code>pufferlib/ocean/drive/drivenet.h</code>: Lightweight C inference network used by the visualizer/demo to mirror the Torch policy outputs.</li> </ul>"},{"location":"simulator/#drive-readme-c-core-notes","title":"Drive README (C core notes)","text":""},{"location":"simulator/#agent-initialization-and-control","title":"Agent initialization and control","text":""},{"location":"simulator/#init_mode","title":"<code>init_mode</code>","text":"<p>Determines which agents are created in the environment.</p> Option Description <code>create_all_valid</code> Create all entities valid at initialization (<code>traj_valid[init_steps] == 1</code>). <code>create_only_controlled</code> Create only those agents that are controlled by the policy."},{"location":"simulator/#control_mode","title":"<code>control_mode</code>","text":"<p>Determines which created agents are controlled by the policy.</p> Option Description <code>control_vehicles</code> (default) Control only validvehicles (not experts, beyond <code>MIN_DISTANCE_TO_GOAL</code>, under <code>MAX_AGENTS</code>). <code>control_agents</code> Control all validagent types (vehicles, cyclists, pedestrians). <code>control_tracks_to_predict</code> (WOMD only) Control agents listed in the <code>tracks_to_predict</code> metadata. <code>control_sdc_only</code> (WOMD only) Control just the self-driving car (SDC)."},{"location":"simulator/#termination-conditions-done","title":"Termination conditions (<code>done</code>)","text":"<p>The <code>goal_behavior</code> argument controls agent behavior after reaching a goal early:</p> <ul> <li><code>goal_behavior=0</code> (default): Agents respawn at their initial position after reaching their goal (last valid log position).</li> <li><code>goal_behavior=1</code>: Agents receive new goals indefinitely after reaching each goal.</li> <li><code>goal_behavior=2</code>: Agents stop after reaching their goal.</li> </ul>"},{"location":"simulator/#logged-performance-metrics","title":"Logged performance metrics","text":"<p>We record multiple performance metrics during training, aggregated over all active agents (alive and controlled). Key metrics include:</p> <ul> <li><code>score</code>: Goals reached cleanly (goal was achieved without collision or going off-road)</li> <li><code>collision_rate</code>: Binary flag (0 or 1) if agent hit another vehicle.</li> <li><code>offroad_rate</code>: Binary flag (0 or 1) if agent left road bounds.</li> <li><code>completion_rate</code>: Whether the agent reached its goal in this episode (even if it collided or went off-road).</li> </ul>"},{"location":"simulator/#metric-aggregation","title":"Metric aggregation","text":"<p>The <code>num_agents</code> parameter in <code>drive.ini</code> defines the total number of agents used to collect experience. At runtime, Puffer uses <code>num_maps</code> to create enough environments to populate the buffer with <code>num_agents</code>, distributing them evenly across <code>num_envs</code>.</p> <p>Because agents are respawned immediately after reaching their goal, they remain active throughout the episode.</p> <p>At the end of each episode (i.e., when <code>timestep == TRAJECTORY_LENGTH</code>), metrics are logged once via:</p> <pre><code>if (env-&gt;timestep == TRAJECTORY_LENGTH) {\n    add_log(env);\n    c_reset(env);\n    return;\n}\n</code></pre> <p>Metrics are normalized and aggregated in <code>vec_log</code> (<code>pufferlib/ocean/env_binding.h</code>). They are averaged over all active agents across all environments. For example, the aggregated collision rate is computed as:</p> \\[ r^{agg}_{\\text{collision}} = \\frac{\\mathbb{I}[\\text{collided in episode}]}{N} \\] <p>where \\(N\\) is the number of controlled agents.</p> <p>Since these metrics do not capture multiple events per agent, we additionally log the average number of collision and off-road events per episode. This is computed as:</p> \\[ c^{avg}_{\\text{collision}} = \\frac{\\text{total number of collision events across all agents and environments}}{N} \\] <p>where \\(N\\) is the total number of controlled agents. For example, an <code>avg_collisions_per_agent</code> value of 4 indicates that, on average, each agent collides four times per episode.</p> <p></p>"},{"location":"simulator/#effect-of-respawning-on-metrics","title":"Effect of respawning on metrics","text":"<p>By default, agents are reset to their initial position when they reach their goal before the episode ends. Upon respawn, <code>respawn_timestep</code> is updated from <code>-1</code> to the current step index. After an agent respawns, all other agents are removed from the environment, so collisions with other agents cannot occur post-respawn.</p> <p></p> <p></p>"},{"location":"train/","title":"Training agents","text":""},{"location":"train/#training","title":"Training","text":""},{"location":"train/#basic-training","title":"Basic training","text":"<p>Launch a training run with Weights &amp; Biases logging: <pre><code>puffer train puffer_drive --wandb --wandb-project \"pufferdrive\"\n</code></pre></p>"},{"location":"train/#environment-configurations","title":"Environment configurations","text":"<p>Default configuration (Waymo maps)</p> <p>The default settings in <code>drive.ini</code> are optimized for:</p> <ul> <li>Training in thousands of Waymo maps</li> <li>Short episodes (91 steps)</li> </ul> <p>Carla maps configuration For training agents to drive indefinitely in larger Carla maps, we recommend modifying <code>drive.ini</code> as follows: <pre><code>[env]\ngoal_speed = 30.0  # Target speed in m/s at the goal. Lower values discourage excessive speeding\ngoal_behavior = 1  # 0: respawn, 1: generate_new_goals, 2: stop\ngoal_target_distance = 25.0  # Distance to new goal when using generate_new_goals\n\n# Episode settings\nepisode_length = 200 # Increase for longer episode horizon\nresample_frequency = 100000 # No resampling needed (there are only a few Carla maps)\ntermination_mode = 0  # 0: terminate at episode_length, 1: terminate after all agents reset\n\n# Map settings\nmap_dir = \"resources/drive/binaries/carla\"\nnum_maps = 1\n</code></pre></p> <p>Note: The default training hyperparameters work well for both configurations and typically don't need adjustment.</p>"},{"location":"train/#controlled-experiments","title":"Controlled experiments","text":"<p>Run parameter sweeps for architecture search or multi-seed experiments: <pre><code>puffer controlled_exp puffer_drive --wandb --wandb-project \"pufferdrive2.0_carla\" --tag speed\n</code></pre></p> <p>Define parameter sweeps in <code>drive.ini</code>: <pre><code>[controlled_exp.env.goal_speed]\nvalues = [10, 20, 30]\n</code></pre></p> <p>This will launch separate training runs for each value in the list, useful for: - Hyperparameter tuning - Architecture search - Running multiple random seeds - Ablation studies</p> <p>You can specify multiple controlled experiment parameters, and the system will iterate through all combinations.</p>"},{"location":"visualizer/","title":"Visualizer","text":"<p>PufferDrive ships a Raylib-based visualizer for replaying scenes, exporting videos, and debugging policies.</p>"},{"location":"visualizer/#dependencies","title":"Dependencies","text":"<p>Install the minimal system packages for headless render/export:</p> <pre><code>sudo apt update\nsudo apt install ffmpeg xvfb\n</code></pre> <p>On environments without sudo, install them into your conda/venv:</p> <pre><code>conda install -c conda-forge xorg-x11-server-xvfb-cos6-x86_64 ffmpeg\n</code></pre>"},{"location":"visualizer/#build","title":"Build","text":"<p>Compile the visualizer binary from the repo root:</p> <pre><code>bash scripts/build_ocean.sh visualize local\n</code></pre> <p>If you need to force a rebuild, remove the cached binary first (<code>rm ./visualize</code>).</p>"},{"location":"visualizer/#run-headless","title":"Run headless","text":"<p>Launch the visualizer with a virtual display and export an <code>.mp4</code>:</p> <pre><code>xvfb-run -s \"-screen 0 1280x720x24\" ./visualize\n</code></pre> <p>Adjust the screen size and color depth as needed. The <code>xvfb-run</code> wrapper allows Raylib to render without an attached display, which is convenient for servers and CI jobs.</p>"},{"location":"wosac/","title":"Waymo Open Sim Agent Challenge (WOSAC) Benchmark","text":"<p>We provide a re-implementation of the Waymo Open Sim Agent Challenge (WOSAC), which measures distributional realism of simulated trajectories compared to logged human trajectories. Our version preserves the original logic and metric weighting but uses PyTorch on GPU for the metrics computation, unlike the original TensorFlow CPU implementation. The code is also simplified for clarity, making it easier to understand, adapt, and extend.</p> <p>Note: In PufferDrive, agents are conditioned on a \"goal\" represented as a single (x, y) position, reflecting that drivers typically have a high-level destination in mind. Evaluating whether an agent matches human distributional properties can be decomposed into: (1) inferring a person's intended direction from context (1 second in WOSAC) and (2) navigating toward that goal in a human-like manner. We focus on the second component, though the evaluation could be adapted to include behavior prediction as in the original WOSAC.</p> <p>[TODO: ADD bar graphs]</p>"},{"location":"wosac/#usage","title":"Usage","text":""},{"location":"wosac/#running-a-single-evaluation-from-a-checkpoint","title":"Running a single evaluation from a checkpoint","text":"<p>The <code>[eval]</code> section in <code>drive.ini</code> contains all relevant configurations. To run the WOSAC eval once:</p> <pre><code>puffer eval puffer_drive --eval.wosac-realism-eval True --load-model-path &lt;your-trained-policy&gt;.pt\n</code></pre> <p>The default configs aim to emulate the WOSAC settings as closely as possible, but you can adjust them:</p> <pre><code>[eval]\nmap_dir = \"resources/drive/binaries/validation\" # Dataset to use\nnum_maps = 100  # Number of maps to run evaluation on. (It will alwasys be the first num_maps maps of the map_dir)\nwosac_num_rollouts = 32      # Number of policy rollouts per scene\nwosac_init_steps = 10        # When to start the simulation\nwosac_control_mode = \"control_wosac\"  # Control the tracks to predict\nwosac_init_mode = \"create_all_valid\"  # Initialize from the tracks to predict\nwosac_goal_behavior = 2      # Stop when reaching the goal\nwosac_goal_radius = 2.0      # Can shrink goal radius for WOSAC evaluation\n</code></pre>"},{"location":"wosac/#log-evals-to-wb-during-training","title":"Log evals to W&amp;B during training","text":"<p>During experimentation, logging key metrics directly to W&amp;B avoids a post-training step. Evaluations can be enabled during training, with results logged under a separate <code>eval/</code> section. The main configuration options:</p> <pre><code>[train]\ncheckpoint_interval = 500    # Set equal to eval_interval to use the latest checkpoint\n\n[eval]\neval_interval = 500          # Run eval every N epochs\nmap_dir = \"resources/drive/binaries/training\"  # Dataset to use\nnum_maps = 20 # Number of maps to run evaluation on. (It will alwasys be the first num_maps maps of the map_dir)\n</code></pre>"},{"location":"wosac/#baselines","title":"Baselines","text":"<p>We provide baselines on a small curated dataset from the WOMD validation set with perfect ground-truth (no collisions or off-road events from labeling mistakes).</p> Method Realism meta-score Kinematic metrics Interactive metrics Map-based metrics minADE ADE Ground-truth (UB) 0.832 0.606 0.846 0.961 0 0 \u03c0_Base self-play RL 0.737 0.319 0.789 0.938 10.834 11.317 SMART-tiny-CLSFT 0.805 0.534 0.830 0.949 1.124 3.123 \u03c0_Random 0.485 0.214 0.657 0.408 6.477 18.286 <p>Table: WOSAC baselines in PufferDrive on 229 selected clean held-out validation scenarios.</p> <p>\u270f\ufe0f Download the dataset from Hugging Face to reproduce these results or benchmark your policy.</p>"},{"location":"wosac/#useful-links","title":"Useful links","text":"<ul> <li>WOSAC challenge and leaderboard</li> <li>Sim agent challenge tutorial</li> <li>Reference paper introducing WOSAC</li> <li>Metrics entry point</li> <li>Log-likelihood estimators</li> <li>Configurations proto file</li> <li>Default sim agent challenge configs</li> </ul>"}]}