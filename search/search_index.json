{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"PufferDrive","text":"<p>PufferDrive is a high-throughput autonomous driving simulator built on PufferLib. Train and evaluate multi-agent driving policies with fast vectorized stepping, streamlined data conversion, and ready-made benchmarks.</p> Start here: install &amp; build See the workflow"},{"location":"#highlights","title":"Highlights","text":"<ul> <li>Multi-agent drive environment with batched stepping for speed.</li> <li>Scripts to convert Waymo Open Motion Dataset JSON into lightweight binaries (CARLA sample map included).</li> <li>Benchmarks for distributional realism and human compatibility.</li> <li>Raylib-based visualizer for local or headless render/export.</li> </ul>"},{"location":"#quick-start","title":"Quick start","text":"<ul> <li>Follow Getting Started to install, build the C extensions, and run <code>puffer train puffer_drive</code>.</li> <li>Consult Simulator for how actions/observations, rewards, and <code>.ini</code> settings map to the underlying C environment and Torch policy.</li> <li>Prepare drive map binaries with the steps in Data.</li> <li>Evaluate a policy with the commands in Evaluation and preview runs with the Visualizer.</li> </ul>"},{"location":"#workflow","title":"Workflow","text":"Step 1 Install &amp; Build <p>Set up the environment, install dependencies, and compile the native extensions.</p> Open guide Step 2 Prepare Data <p>Download WOMD/GPUDrive datasets from Hugging Face and convert to map binaries.</p> Open guide Step 3 Train &amp; Evaluate <p>Run `puffer train puffer_drive` and score policies with WOSAC and human-replay benchmarks.</p> Open guide"},{"location":"#repository-layout","title":"Repository layout","text":"<ul> <li><code>pufferlib/ocean/drive</code>: Drive environment implementation and map processing utilities.</li> <li><code>resources/drive/binaries</code>: Expected location for compiled map binaries (outputs of the data conversion step).</li> <li><code>scripts/build_ocean.sh</code>: Helper for building the Raylib visualizer and related binaries.</li> <li><code>examples</code>, <code>tests</code>, <code>experiments</code>: Reference usage, checks, and research scripts that pair with the docs pages.</li> </ul>"},{"location":"benchmark-readme/","title":"Waymo Open Sim Agent Challenge (WOSAC) benchmark","text":""},{"location":"benchmark-readme/#usage","title":"Usage","text":"<p>WOSAC evaluation with random policy <pre><code>puffer eval puffer_drive --eval.wosac-realism-eval True\n</code></pre></p> <p>WOSAC evaluation with your checkpoint <pre><code>puffer eval puffer_drive --eval.wosac-realism-eval True --load-model-path &lt;your-trained-policy&gt;.pt\n</code></pre></p>"},{"location":"benchmark-readme/#links","title":"Links","text":"<ul> <li>Challenge and leaderboard</li> <li>Sim agent challenge tutorial</li> <li>Reference paper introducing WOSAC</li> <li>Metrics entry point</li> <li>Log-likelihood estimators</li> <li>Configurations proto file default sim agent challenge configs</li> </ul>"},{"location":"benchmark-readme/#implementation","title":"Implementation","text":"<ul> <li>For the sim agent challenge we compute the log likelihood with <code>aggregate_objects=False</code>, which means that we use <code>_log_likelihood_estimate_timeseries_agent_level()</code></li> <li>As such, the interpretation is as follows:</li> </ul> <p>Steps [for every scene]: 1. Rollout policy in environment K times \u2192 (n_agents, n_rollouts, n_steps) 2. Obtain log data \u2192 (n_agents, k, n_steps) 3. Obtain features from (x, y, z, heading tuples) 4. Compute log-likelihood metrics from features     - a. Flatten across time (assume independence) \u2192 (n_agents, n_rollouts * n_steps)     - b. Use the per-agent simulated features to construct a probability distribution     - c. Take the per-agent ground-truth values and find the bin that is closest for each     - d. Take log of the probability for each bin \u2192 (n_agents, n_steps) 5. Likelihood score is exp(sum(log_probs)/n_steps) \u2192 (n_agents, 1) \\in [0, 1]</p>"},{"location":"benchmark-readme/#notes","title":"Notes","text":"<ul> <li> <p>Currently, only kinematics realism score is implemented. Next steps would be to add the interactive realism score, and the map realism score:</p> <ul> <li>Interactive realism score: requires grouping agents per scenario, and computing pairwise distances between agents over time.</li> <li>Map realism score: requires access to the map and computing offroad status.</li> </ul> <p>Those two scores might require heavy computations, so we will consider reimplementing all the metrics in torch.</p> </li> <li> <p>About the time-independence assumption:</p> <ol> <li> <p>This is the assumption used in the official WOSAC evaluation, their argument is that it would give more flexibility to the sim agents models:</p> <p>Given the time series nature of simulation data, two choices emerge for how to treat samples over multiple timesteps for a given object for a given run segment: to treat them as time-independent or time-dependent samples. In the latter case, users would be expected to not only reconstruct the general behaviors present in the logged data in one rollout, but also recreate those behaviors over the exact same time intervals. To allow more flexibility in agent behavior, we use the former formulation when computing NLLs, defining each component metric m as an average (in log-space) over the time-axis, masked by validity.</p> </li> <li> <p>However this will lead to the score of a perfect logged oracle being inferior to 1.0, and makes it less interpretable. Here are the scores of a logged oracle using the time-independence assumption (setup: 1024 agents, 48 rollouts):</p> <p><pre><code>Linear speed: 0.5640\nLinear acceleration: 0.4658\nAngular speed: 0.5543\nAngular acceleration: 0.6589\n\nKinematics realism score: 0.5607\n</code></pre>     These scores go to 1.0 if we use the time-dependent estimator, execpt for the smoothing factor that is used to avoid bins with 0 probability.</p> </li> </ol> <p>Using the time-dependent estimator means generating n_steps histograms per agent, using num_rollouts samples per histogram, while time-independence means generating one histogram per agent using n_rollouts * n_steps samples. With the speed of PufferDrive,  we might be able to increase n_rollouts to have more samples per histogram.</p> </li> </ul>"},{"location":"data/","title":"Data","text":"<p>PufferDrive consumes map binaries generated from the Waymo Open Motion Dataset (WOMD) JSON files. This page covers where to obtain data and how to convert it into the binary format expected by the simulator.</p>"},{"location":"data/#download-options","title":"Download options","text":"<ul> <li>Mini dataset: GPUDrive_mini with ~1k training files for quick experiments.</li> <li>Full dataset: GPUDrive with ~100k scenes for large-scale training.</li> <li>Additional compatible sources: ScenarioMax exports JSON in the same format.</li> <li>Included CARLA maps: CARLA exports live in <code>data_utils/carla/carla</code>.</li> </ul>"},{"location":"data/#download-via-hugging-face","title":"Download via Hugging Face","text":"<p>Install the CLI once:</p> <pre><code>uv pip install -U \"huggingface_hub[cli]\"\n</code></pre> <p>Download the mini dataset:</p> <pre><code>huggingface-cli download EMERGE-lab/GPUDrive_mini \\\n  --repo-type dataset \\\n  --local-dir data/processed/training \\\n  --include \"training/*\"\n</code></pre> <p>Download the full dataset:</p> <pre><code>huggingface-cli download EMERGE-lab/GPUDrive \\\n  --repo-type dataset \\\n  --local-dir data/processed/training \\\n  --include \"training/*\"\n</code></pre> <p>Place raw JSON files under <code>data/processed/training</code> (default location read by the conversion script).</p>"},{"location":"data/#convert-json-to-map-binaries","title":"Convert JSON to map binaries","text":"<p>The conversion script writes compact <code>.bin</code> maps to <code>resources/drive/binaries</code>:</p> <pre><code>python pufferlib/ocean/drive/drive.py\n</code></pre> <p>Notes: - The script iterates every JSON file in <code>data/processed/training</code> and emits <code>map_XXX.bin</code> files. - <code>resources/drive/binaries/map_000.bin</code> ships with the repo for quick smoke tests; generate additional bins for training/eval. - If you want to point at a different dataset location or limit the number of maps, adjust <code>process_all_maps</code> in <code>pufferlib/ocean/drive/drive.py</code> before running.</p>"},{"location":"data/#map-binary-format-reference","title":"Map binary format reference","text":"<p>The simulator reads the compact binary layout produced by <code>save_map_binary</code> in <code>pufferlib/ocean/drive/drive.py</code> and parsed by <code>load_map_binary</code> in <code>pufferlib/ocean/drive/drive.h</code>:</p> <ul> <li>Header: <code>sdc_track_index</code> (int), <code>num_tracks_to_predict</code> (int) followed by that many <code>track_index</code> ints, <code>num_objects</code> (int), <code>num_roads</code> (int).</li> <li>Objects (vehicles/pedestrians/cyclists): For each object, the writer stores <code>scenario_id</code> (<code>unique_map_id</code> passed to <code>load_map</code>), <code>type</code> (<code>1</code> vehicle, <code>2</code> pedestrian, <code>3</code> cyclist), <code>id</code>, <code>array_size</code> (<code>TRAJECTORY_LENGTH = 91</code>), positions <code>x/y/z[91]</code>, velocities <code>vx/vy/vz[91]</code>, <code>heading[91]</code>, <code>valid[91]</code>, and scalars <code>width/length/height</code>, <code>goalPosition (x, y, z)</code>, <code>mark_as_expert</code> (int). Missing trajectory entries are zero-padded by the converter.</li> <li>Road elements: Each road entry stores <code>scenario_id</code>, a remapped <code>type</code> (<code>4</code> lane, <code>5</code> road line, <code>6</code> road edge, <code>7</code> stop sign, <code>8</code> crosswalk, <code>9</code> speed bump, <code>10</code> driveway), <code>id</code>, <code>array_size</code> (#points), then <code>x/y/z</code> arrays of that length and scalars <code>width/length/height</code>, <code>goalPosition</code>, <code>mark_as_expert</code>. <code>save_map_binary</code> also simplifies long polylines (<code>len(geometry) &gt; 10</code> and <code>type &lt;= 16</code>) with a 0.1 area threshold to keep files small.</li> <li>Control hints: <code>tracks_to_predict</code> and <code>mark_as_expert</code> influence which agents are controllable (<code>control_mode</code> in the simulator) versus replayed as experts or static actors (<code>set_active_agents</code> in <code>drive.h</code>).</li> </ul> <p>Refer to Simulator for how the binaries are consumed during resets, observation construction, and reward logging.</p>"},{"location":"data/#verifying-data-availability","title":"Verifying data availability","text":"<ul> <li>After conversion, <code>ls resources/drive/binaries | head</code> should show numbered <code>.bin</code> files.</li> <li>If you see <code>Required directory resources/drive/binaries/map_000.bin not found</code> during training, rerun the conversion or check paths.</li> <li>With binaries in place, run <code>puffer train puffer_drive</code> from Getting Started as a smoke test that the build, data, and bindings are wired together.</li> <li>To inspect the binary output, convert a single JSON file with <code>load_map(&lt;json&gt;, &lt;id&gt;, &lt;output_path&gt;)</code> inside <code>drive.py</code>.</li> </ul>"},{"location":"data/#waymo-map-editor","title":"Waymo map editor","text":"<p>See WOMD Editor for a browser-based workflow to inspect, edit, and export Waymo/ScenarioMax JSON into the <code>.bin</code> format consumed by the simulator.</p>"},{"location":"evaluation/","title":"Evaluation","text":"<p>Benchmarks are provided to measure how closely agents match real-world driving behavior and how well they pair with human trajectories.</p>"},{"location":"evaluation/#sanity-maps","title":"Sanity maps","text":"<p>Quickly test the training on curated, lightweight scenarios without downloading the full dataset. Each sanity map tests a specific behavior.</p> <pre><code>puffer sanity puffer_drive --wandb --wandb-name sanity-demo --sanity-maps forward_goal_in_front s_curve\n</code></pre> <p>Or run them all at once:</p> <pre><code>puffer sanity puffer_drive --wandb --wandb-name sanity-all\n</code></pre> <ul> <li>Tip:Turn learning-rate annealing off for these short runs (<code>--train.anneal_lr False</code>) to keep the sanity checks from decaying the optimizer mid-run.</li> </ul> <p>Available maps: - <code>forward_goal_in_front</code>: Straight approach to a goal in view. - <code>reverse_goal_behind</code>: Backward start with a behind-the-ego goal. - <code>two_agent_forward_goal_in_front</code>: Two agents advancing to forward goals. - <code>two_agent_reverse_goal_behind</code>: Two agents reversing to rear goals. - <code>simple_turn</code>: Single, gentle turn to a nearby goal. - <code>s_curve</code>: S-shaped path with alternating curvature. - <code>u_turn</code>: U-shaped turn to a goal behind the start. - <code>one_or_two_point_turn</code>: Tight turn requiring a small reversal. - <code>three_or_four_point_turn</code>: Even tighter turn needing multiple reversals. - <code>goal_out_of_sight</code>: Goal starts without direct path; needs some planning.</p> <p></p>"},{"location":"evaluation/#wosac-distributional-realism","title":"WOSAC distributional realism","text":"<p>Evaluate how realistic your policy behaves compared to the Waymo Open Sim Agents Challenge (WOSAC):</p> <pre><code>puffer eval puffer_drive --eval.wosac-realism-eval True\n</code></pre> <p>Add <code>--load-model-path &lt;path_to_checkpoint&gt;.pt</code> to score a trained policy instead of a random baseline. See WOSAC Benchmark for the metric pipeline and links to the official configs.</p>"},{"location":"evaluation/#human-compatibility","title":"Human-compatibility","text":"<p>Test how a policy coexists with human-controlled agents:</p> <pre><code>puffer eval puffer_drive --eval.human-replay-eval True --load-model-path &lt;path_to_checkpoint&gt;.pt\n</code></pre> <p>During this evaluation the self-driving car (SDC) is controlled by your policy while other agents replay log data.</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This page walks through installing PufferDrive from source, building the native extensions, and running a first training job.</p>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.9+ with a virtual environment manager (<code>uv</code>, <code>venv</code>, or <code>conda</code>).</li> <li>A C/C++ toolchain for building the bundled extensions (GCC/Clang + make).</li> <li>PyTorch installed inside your environment (pick the CPU/GPU wheel that matches your setup).</li> </ul>"},{"location":"getting-started/#installation","title":"Installation","text":"<p>Clone and set up an isolated environment:</p> <pre><code>git clone https://github.com/Emerge-Lab/PufferDrive.git\ncd PufferDrive\nuv venv .venv &amp;&amp; source .venv/bin/activate\nuv pip install -e .\n</code></pre> <p>Build the C extensions in place:</p> <p><pre><code>python setup.py build_ext --inplace --force\n</code></pre> Run this with your virtual environment activated so the compiled extension links against the correct Python.</p>"},{"location":"getting-started/#when-to-rebuild-the-extension","title":"When to rebuild the extension","text":"<ul> <li>Re-run <code>python setup.py build_ext --inplace --force</code> after changing any C/Raylib sources in <code>pufferlib/ocean/drive</code> (e.g., <code>drive.c</code>, <code>drive.h</code>, <code>binding.c</code>, <code>visualize.c</code>) or after pulling upstream changes that touch those files. This regenerates the <code>binding.cpython-*.so</code> used by <code>Drive</code>.</li> <li>Pure Python edits (training scripts, docs, data utilities) do not require a rebuild; just restart your Python process.</li> </ul>"},{"location":"getting-started/#verify-the-setup","title":"Verify the setup","text":"<p>Once map binaries are available (see Data), launch a quick training run to confirm the environment, data, and bindings are wired up correctly:</p> <pre><code>puffer train puffer_drive\n</code></pre> <p>If map binaries are missing, follow the steps in Data to generate them before training. See Visualizer for rendering runs and Evaluation for benchmark commands.</p>"},{"location":"getting-started/#logging-with-weights-biases","title":"Logging with Weights &amp; Biases","text":"<p>Enable W&amp;B logging with the built-in CLI flags (the package is already a dependency in <code>setup.py</code>):</p> <pre><code>puffer train puffer_drive --wandb --wandb-project pufferdrive --wandb-group local-dev\n</code></pre> <ul> <li>Add <code>--wandb</code> to turn on logging; <code>--wandb-project</code> and <code>--wandb-group</code> set the destination in W&amp;B.</li> <li>Checkpoint uploads and evaluation helpers (<code>pufferlib/utils.py</code>) will log WOSAC/human-replay metrics and rendered videos when W&amp;B is enabled.</li> </ul>"},{"location":"pufferdrive-2.0/","title":"PufferDrive 2.0: A fast and friendly driving simulator for training and evaluating RL agents","text":"<p>Daphne Cornelisse\u00b9\u00b7, Spencer Cheng\u00b2\u00b7, Pragnay Mandavilli\u00b9, Julian Hunt\u00b9, Kevin Joseph\u00b9, Wa\u00ebl Doulazmi\u00b3, Eugene Vinitsky\u00b9</p> <p>\u00b9Emerge Lab at NYU | \u00b2Puffer.ai | \u00b3Valeo | *Shared first authorship</p> <p>December 12, 2025</p> <p>We introduce PufferDrive 2.0, a fast and easy-to-use driving simulator for reinforcement learning. It supports training at up to 300,000 steps per second on a single GPU, enabling agents to reach strong performance in just a few hours. Evaluation and visualization run directly in the browser.</p> <p>This post outlines the design goals, highlights the main features, and shows what works out of the box. We conclude with a brief roadmap.</p>"},{"location":"pufferdrive-2.0/#introduction-and-history","title":"Introduction and history","text":"<p>Deep reinforcement learning algorithms, such as PPO, are highly effective in the billion-sample regime. A consistent finding across domains is that, given sufficient environmental signal and enough data, any precisely specified objective can, in principle, be optimized.</p> <p>This shifts the primary bottleneck to simulation. The faster we can generate high-quality experience, the more reliably we can apply RL to hard real-world problems, such as autonomous navigation in dynamic, unstructured environments.<sup>1</sup></p> <p>Over the past few years, several simulators have demonstrated that large-scale self-play can work for driving. Below, we summarize this progression and explain how it led to PufferDrive 2.0.</p>"},{"location":"pufferdrive-2.0/#early-results-with-self-play-rl-in-autonomous-driving","title":"Early results with self-play RL in autonomous driving","text":"<p>Nocturne was the first paper to show that self-play RL could work for driving at scale. Using maps from the Waymo Open Motion Dataset (WOMD), PPO agents achieved around an 80% goal-reaching rate without any human data.</p> <p>The main limitation was speed. Nocturne ran at roughly 2,000 steps per second, leading to multi-day training times and a complex setup process.</p> <p>The results were promising, but it was clear that scale was a major constraint.</p>"},{"location":"pufferdrive-2.0/#scaling-up","title":"Scaling up","text":"<p>Subsequent work showed what becomes possible when scale is no longer the bottleneck.</p> <ul> <li>Gigaflow demonstrated that large-scale self-play alone can produce robust, naturalistic driving. Using a highly batched simulator, it trained on the equivalent of decades of driving experience per hour and achieved state-of-the-art performance across multiple autonomous driving benchmarks\u2014without using any human data.</li> <li>GPUDrive, built on Madrona, showed that similar behavior could be learned in about one day on a single consumer GPU, using a simple reward function and a standard PPO implementation.</li> </ul> <p>These empirical results support the hypothesis that robust autonomous driving policies can be trained in the billion-sample regime without any human data.</p> <p> Figure 1: Progression of RL-based driving simulators. Left: end-to-end training throughput on an NVIDIA RTX 4080, counting only transitions collected by learning policy agents (excluding padding agents). Right: wall-clock time (log scale) required to reach an 80% goal-reaching rate. This metric captures both simulation speed and algorithmic efficiency.</p>"},{"location":"pufferdrive-2.0/#from-gpudrive-to-pufferdrive","title":"From GPUDrive to PufferDrive","text":"<p>While GPUDrive delivered impressive raw simulation speed, end-to-end training throughput of around 50K steps per second remained a limiting factor. This was particularly true on large maps such as CARLA. Memory layout and batching overheads, rather than simulation fidelity, became the dominant constraints.</p> <p>Faster end-to-end training is critical because it enables tighter debugging loops, broader experimentation, and faster scientific and engineering progress. This led directly to the development of PufferDrive.</p> <p>We partnered with Spencer Cheng from Puffer.ai to rebuild the system around the principles of PufferLib. Spencer reimplemented GPUDrive. The result was PufferDrive 1.0, reaching approximately 200,000 steps per second on a single GPU and scaling linearly across multiple GPUs. Training agents to solve 10,000 maps from the Waymo datset took roughly 24 hours with GPUDrive. With PufferDrive, the same results could now be reproduced in about 2 hours.</p>"},{"location":"pufferdrive-2.0/#pufferdrive-20","title":"PufferDrive 2.0","text":"<p>PufferDrive 2.0 builds on this foundation and [TODO]:</p> <ul> <li>Built-in evaluations, including a standard benchmark</li> <li>Support for multiple real-world datasets (WOMD, Carla)</li> <li>Speed improvement (200K -&gt; 300K)</li> <li>Extended browser-based visualization and analysis tools</li> </ul> <p>To our knowledge, PufferDrive 2.0 is among the fastest open-source driving simulators available today, while remaining accessible to new users.</p>"},{"location":"pufferdrive-2.0/#highlights","title":"Highlights","text":"<p>TODO</p>"},{"location":"pufferdrive-2.0/#roadmap","title":"Roadmap","text":"<p>TODO</p>"},{"location":"pufferdrive-2.0/#citation","title":"Citation","text":"<p>If you use PufferDrive in your research, please cite: <pre><code>@software{pufferdrive2024github,\n  author = {Daphne Cornelisse* and Spencer Cheng* and Pragnay Mandavilli and Julian Hunt and Kevin Joseph and Wa\u00ebl Doulazmi and Eugene Vinitsky},\n  title = {{PufferDrive}: A Fast and Friendly Driving Simulator for Training and Evaluating {RL} Agents},\n  url = {https://github.com/Emerge-Lab/PufferDrive},\n  version = {2.0.0},\n  year = {2025},\n}\n</code></pre> *Equal contribution</p> <ol> <li> <p>A useful parallel comes from the early days of computing. In the 1970s and 1980s, advances in semiconductor manufacturing and microprocessor design\u2014such as Intel\u2019s 8080 and 80286 chips\u2014dramatically reduced computation costs and increased speed. This made iterative software development accessible and enabled entirely new ecosystems of applications, ultimately giving rise to the personal computer. Multi-agent RL faces a similar bottleneck today: progress is limited by the cost and speed of experience collection. Fast, affordable simulation with integrated RL algorithms may play a similar catalytic role, enabling solutions that were previously out of reach.\u00a0\u21a9</p> </li> </ol>"},{"location":"simulator/","title":"Simulator Guide","text":"<p>Deep dive into how the Drive environment is wired, what it expects as inputs, and how observations/actions/configs are shaped. The environment entrypoint is <code>pufferlib/ocean/drive/drive.py</code>, which wraps the C core in <code>pufferlib/ocean/drive/drive.h</code> via <code>binding.c</code>.</p>"},{"location":"simulator/#runtime-inputs-and-lifecycle","title":"Runtime inputs and lifecycle","text":"<ul> <li>Map binaries: The environment scans <code>resources/drive/binaries</code> for <code>map_*.bin</code> files and requires at least one to load. Keep <code>num_maps</code> no larger than what is present on disk. During vectorized setup, <code>binding.shared</code> samples maps until it accumulates at least <code>num_agents</code> controllable entities, skipping maps with no valid agents (<code>set_active_agents</code> in <code>drive.h</code>).</li> <li>Episode length: Default <code>scenario_length = 91</code> to match the Waymo logs (trajectory data is 91 steps), but you can set <code>env.scenario_length</code> (CLI or <code>.ini</code>) to any positive value. Metrics are logged and <code>c_reset</code> is called when <code>timestep == scenario_length</code>.</li> <li>Resampling maps: Python-side <code>Drive.step</code> reinitializes the vectorized environments every <code>resample_frequency</code> steps (default <code>910</code>, ~10 episodes) with fresh map IDs and seeds.</li> <li>Initialization controls:</li> <li><code>init_steps</code> starts agents from a later timestep in the logged trajectory.</li> <li><code>init_mode</code> (<code>create_all_valid</code> vs <code>create_only_controlled</code>) decides which logged actors are instantiated at reset.</li> <li><code>control_mode</code> (<code>control_vehicles</code>, <code>control_agents</code>, <code>control_tracks_to_predict</code>, <code>control_sdc_only</code>) selects which instantiated actors are policy-controlled. Non-controlled actors can still appear as static or expert replay agents.</li> <li><code>goal_behavior</code> chooses what happens on goal reach (<code>0</code> respawn at start pose, <code>1</code> sample new lane-following goals via the lane topology graph, <code>2</code> stop in place). <code>goal_radius</code> sets the completion threshold in meters.</li> </ul> <p>See Data for how to produce the <code>.bin</code> inputs, including the binary layout.</p>"},{"location":"simulator/#actions-and-dynamics","title":"Actions and dynamics","text":"<ul> <li>Action types (<code>env.action_type</code>):</li> <li><code>discrete</code> (default): classic dynamics use a single <code>MultiDiscrete([7*13])</code> index decoded into acceleration (<code>ACCELERATION_VALUES</code>) and steering (<code>STEERING_VALUES</code>); jerk dynamics use <code>MultiDiscrete([4, 3])</code> over <code>JERK_LONG</code>/<code>JERK_LAT</code>.</li> <li><code>continuous</code>: a 2-D Box in <code>[-1, 1]</code>. Classic scales to the max accel/steer magnitudes used in the discrete table. Jerk scales asymmetrically: negative values reach up to <code>-15 m/s^3</code> braking, positives up to <code>4 m/s^3</code> acceleration, lateral jerk up to <code>\u00b14 m/s^3</code>.</li> <li>Dynamics models (<code>env.dynamics_model</code>):</li> <li><code>classic</code>: bicycle model integrating accel/steer with <code>dt</code> (default <code>0.1</code>).</li> <li><code>jerk</code>: integrates longitudinal/lateral jerk into accel, then into velocity/pose with steering limited to <code>\u00b10.55 rad</code>. Speeds are clipped to <code>[0, 20] m/s</code>.</li> </ul>"},{"location":"simulator/#observation-space","title":"Observation space","text":"<p>Shape is <code>ego_features + 63 * 7 + 200 * 7</code> = <code>1848</code> for classic dynamics (<code>ego_features = 7</code>) or <code>1851</code> for jerk dynamics (<code>ego_features = 10</code>). Computed in <code>compute_observations</code> (<code>drive.h</code>):</p> <ul> <li>Ego block (classic):</li> <li>Goal position in ego frame (x, y) scaled by <code>0.005</code> (~200 m range to 1.0)</li> <li>Ego speed / <code>MAX_SPEED</code> (100 m/s)</li> <li>Width / <code>MAX_VEH_WIDTH</code> (15 m)</li> <li>Length / <code>MAX_VEH_LEN</code> (30 m)</li> <li>Collision flag (1 if the agent collided this step)</li> <li>Respawn flag (1 if the agent was respawned this episode)</li> <li>Ego block additions (jerk dynamics model only):</li> <li>Steering angle / \u03c0</li> <li>Longitudinal acceleration normalized to <code>[-15, 4]</code></li> <li>Lateral acceleration normalized to <code>[-4, 4]</code></li> <li>Respawn flag (index 9)</li> <li>Partner blocks: Up to 63 other agents (active first, then static experts) within 50 m. Each uses 7 values: relative (x, y) in ego frame scaled by <code>0.02</code>, width/length normalized as above, relative heading encoded as <code>(cos \u0394\u03b8, sin \u0394\u03b8)</code>, and speed / <code>MAX_SPEED</code>. Zero-padded when fewer neighbors are present or when agents are in respawn.</li> <li>Road blocks: Up to 200 nearby road segments pulled from a precomputed grid (<code>vision_range = 21</code>). Each entry stores relative midpoint (x, y) scaled by <code>0.02</code>, segment length / <code>MAX_ROAD_SEGMENT_LENGTH</code> (100 m), width / <code>MAX_ROAD_SCALE</code> (100), <code>(cos, sin)</code> of the segment direction in ego frame, and a type ID (<code>ROAD_LANE</code>..<code>DRIVEWAY</code> stored as <code>0..6</code>). Remaining slots are zero-padded.</li> </ul>"},{"location":"simulator/#rewards-termination-and-metrics","title":"Rewards, termination, and metrics","text":"<ul> <li>Per-step rewards (<code>c_step</code>):</li> <li>Collision with another actor: <code>reward_vehicle_collision</code> (default <code>-0.5</code>)</li> <li>Off-road (road-edge intersection): <code>reward_offroad_collision</code> (default <code>-0.2</code>)</li> <li>Goal reached: <code>reward_goal</code> (default <code>1.0</code>) or <code>reward_goal_post_respawn</code> after a respawn</li> <li>Optional ADE shaping: <code>reward_ade * avg_displacement_error</code>, where ADE is accumulated in <code>compute_agent_metrics</code></li> <li>Termination: No early truncation; episodes roll to <code>scenario_length</code> steps. If <code>goal_behavior</code> is respawn, <code>respawn_agent</code> resets the pose and marks <code>respawn_timestep</code> so the respawn flag shows up in observations.</li> <li>Logged metrics (<code>add_log</code> aggregates over all active agents across envs):</li> <li><code>score</code>: reached goal without collision/off-road</li> <li><code>collision_rate</code> / <code>offroad_rate</code>: fraction of agents with \u22651 event in the episode</li> <li><code>avg_collisions_per_agent</code> / <code>avg_offroad_per_agent</code>: counts per agent, capturing repeated events</li> <li><code>completion_rate</code>: reached goal (even if collided/off-road); <code>dnf_rate</code>: clean but never reached goal</li> <li><code>lane_alignment_rate</code>, <code>avg_displacement_error</code>, <code>num_goals_reached</code>, plus counts of active/static/expert agents</li> </ul> <p><code>collision_behavior</code>, <code>offroad_behavior</code>, <code>reward_vehicle_collision_post_respawn</code>, and <code>spawn_immunity_timer</code> are parsed from the INI but currently unused in the stepping logic.</p>"},{"location":"simulator/#configuration-files-ini","title":"Configuration files (<code>.ini</code>)","text":"<p><code>pufferlib/config/default.ini</code> supplies global defaults. Environment-specific overrides live in <code>pufferlib/config/ocean/drive.ini</code> and are loaded first when you run <code>puffer train puffer_drive</code>; CLI flags (e.g., <code>--env.num-maps 128</code>) override both.</p> <p>Key sections in <code>pufferlib/config/ocean/drive.ini</code>: - [env]: Simulator knobs: <code>num_agents</code> (policy slots, C core cap 64), <code>num_maps</code>, <code>scenario_length</code>, <code>resample_frequency</code>, <code>action_type</code>, <code>dynamics_model</code>, rewards, <code>goal_radius</code>, <code>goal_behavior</code>, <code>init_steps</code>, <code>init_mode</code>, <code>control_mode</code>; rendering toggles <code>render</code>, <code>render_interval</code>, <code>obs_only</code>, <code>show_grid</code>, <code>show_lasers</code>, <code>show_human_logs</code>, <code>render_map</code>. - [vec]: Vectorization sizing (<code>num_envs</code>, <code>num_workers</code>, <code>batch_size</code>; backend defaults to multiprocessing). - [policy]/[rnn]: Model widths for the Torch policy (<code>input_size</code>, <code>hidden_size</code>) and optional LSTM wrapper. - [train]: PPO-style hyperparameters (timesteps, learning rate, clipping, batch/minibatch, BPTT horizon, optimizer choice) merged with any unspecified defaults from <code>pufferlib/config/default.ini</code>. - [eval]: WOSAC/human-replay switches and sizing (<code>eval.wosac_*</code>, <code>eval.human_replay_*</code>) mapped directly to the <code>Drive</code> kwargs in evaluation subprocesses.</p>"},{"location":"simulator/#model-overview","title":"Model overview","text":"<p>Defined in <code>pufferlib/ocean/torch.py:Drive</code>: - Three MLP encoders (ego, partners, roads) with LayerNorm. Partner and road encodings are max-pooled across instances. - Concatenated embedding \u2192 GELU \u2192 linear to <code>hidden_size</code>, then split into actor/value heads. - Discrete actions are emitted as logits per dimension (<code>MultiDiscrete</code>), continuous actions as Gaussian parameters (<code>softplus</code> std). Value head is a single linear output. - <code>Recurrent = pufferlib.models.LSTMWrapper</code> can wrap the policy using the <code>rnn</code> config entries; otherwise the policy is feed-forward.</p>"},{"location":"simulator/#drive-source-files-what-lives-where","title":"Drive source files (what lives where)","text":"<ul> <li><code>pufferlib/ocean/drive/drive.py</code>: Python Gymnasium-style wrapper that sets up buffers, validates map availability, seeds the C core via <code>binding.env_init</code>, and handles map resampling.</li> <li><code>pufferlib/ocean/drive/drive.h</code>: Main C implementation of stepping, observations, rewards/metrics, grid map, lane graph, and collision checking.</li> <li><code>pufferlib/ocean/drive/binding.c</code>: Python C-extension glue that exposes <code>Drive</code> to Python, handles shared buffer setup, logging, and reading the <code>.ini</code> config.</li> <li><code>pufferlib/ocean/drive/visualize.c</code>: Raylib-based renderer used by the <code>visualize</code> binary and training video exports.</li> <li><code>pufferlib/ocean/drive/drive.c</code>: Small C demo/perf harness and network parity test runner for the C policy head.</li> <li><code>pufferlib/ocean/drive/drivenet.h</code>: Lightweight C inference network used by the visualizer/demo to mirror the Torch policy outputs.</li> </ul>"},{"location":"simulator/#drive-readme-c-core-notes","title":"Drive README (C core notes)","text":""},{"location":"simulator/#agent-initialization-and-control","title":"Agent initialization and control","text":""},{"location":"simulator/#init_mode","title":"<code>init_mode</code>","text":"<p>Determines which agents are created in the environment.</p> Option Description <code>create_all_valid</code> Create all entities valid at initialization (<code>traj_valid[init_steps] == 1</code>). <code>create_only_controlled</code> Create only those agents that are controlled by the policy."},{"location":"simulator/#control_mode","title":"<code>control_mode</code>","text":"<p>Determines which created agents are controlled by the policy.</p> Option Description <code>control_vehicles</code> (default) Control only valid vehicles (not experts, beyond <code>MIN_DISTANCE_TO_GOAL</code>, under <code>MAX_AGENTS</code>). <code>control_agents</code> Control all valid agent types (vehicles, cyclists, pedestrians). <code>control_tracks_to_predict</code> (WOMD only) Control agents listed in the <code>tracks_to_predict</code> metadata."},{"location":"simulator/#termination-conditions-done","title":"Termination conditions (<code>done</code>)","text":"<p>Episodes are never truncated before reaching <code>episode_len</code>. The <code>goal_behavior</code> argument controls agent behavior after reaching a goal early:</p> <ul> <li><code>goal_behavior=0</code> (default): Agents respawn at their initial position after reaching their goal (last valid log position).</li> <li><code>goal_behavior=1</code>: Agents receive new goals indefinitely after reaching each goal.</li> <li><code>goal_behavior=2</code>: Agents stop after reaching their goal.</li> </ul>"},{"location":"simulator/#logged-performance-metrics","title":"Logged performance metrics","text":"<p>We record multiple performance metrics during training, aggregated over all active agents (alive and controlled). Key metrics include:</p> <ul> <li><code>score</code>: Goals reached cleanly (goal was achieved without collision or going off-road)</li> <li><code>collision_rate</code>: Binary flag (0 or 1) if agent hit another vehicle.</li> <li><code>offroad_rate</code>: Binary flag (0 or 1) if agent left road bounds.</li> <li><code>completion_rate</code>: Whether the agent reached its goal in this episode (even if it collided or went off-road).</li> </ul>"},{"location":"simulator/#metric-aggregation","title":"Metric aggregation","text":"<p>The <code>num_agents</code> parameter in <code>drive.ini</code> defines the total number of agents used to collect experience. At runtime, Puffer uses <code>num_maps</code> to create enough environments to populate the buffer with <code>num_agents</code>, distributing them evenly across <code>num_envs</code>.</p> <p>Because agents are respawned immediately after reaching their goal, they remain active throughout the episode.</p> <p>At the end of each episode (i.e., when <code>timestep == TRAJECTORY_LENGTH</code>), metrics are logged once via:</p> <pre><code>if (env-&gt;timestep == TRAJECTORY_LENGTH) {\n    add_log(env);\n    c_reset(env);\n    return;\n}\n</code></pre> <p>Metrics are normalized and aggregated in <code>vec_log</code> (<code>pufferlib/ocean/env_binding.h</code>). They are averaged over all active agents across all environments. For example, the aggregated collision rate is computed as:</p> \\[ r^{agg}_{\\text{collision}} = \\frac{\\mathbb{I}[\\text{collided in episode}]}{N} \\] <p>where \\(N\\) is the number of controlled agents.</p> <p>Since these metrics do not capture multiple events per agent, we additionally log the average number of collision and off-road events per episode. This is computed as:</p> \\[ c^{avg}_{\\text{collision}} = \\frac{\\text{total number of collision events across all agents and environments}}{N} \\] <p>where \\(N\\) is the total number of controlled agents. For example, an <code>avg_collisions_per_agent</code> value of 4 indicates that, on average, each agent collides four times per episode.</p> <p></p>"},{"location":"simulator/#effect-of-respawning-on-metrics","title":"Effect of respawning on metrics","text":"<p>By default, agents are reset to their initial position when they reach their goal before the episode ends. Upon respawn, <code>respawn_timestep</code> is updated from <code>-1</code> to the current step index. After an agent respawns, all other agents are removed from the environment, so collisions with other agents cannot occur post-respawn.</p> <p></p> <p></p>"},{"location":"visualizer/","title":"Visualizer","text":"<p>PufferDrive ships a Raylib-based visualizer for replaying scenes, exporting videos, and debugging policies.</p>"},{"location":"visualizer/#dependencies","title":"Dependencies","text":"<p>Install the minimal system packages for headless render/export:</p> <pre><code>sudo apt update\nsudo apt install ffmpeg xvfb\n</code></pre> <p>On environments without sudo, install them into your conda/venv:</p> <pre><code>conda install -c conda-forge xorg-x11-server-xvfb-cos6-x86_64 ffmpeg\n</code></pre>"},{"location":"visualizer/#build","title":"Build","text":"<p>Compile the visualizer binary from the repo root:</p> <pre><code>bash scripts/build_ocean.sh visualize local\n</code></pre> <p>If you need to force a rebuild, remove the cached binary first (<code>rm ./visualize</code>).</p>"},{"location":"visualizer/#run-headless","title":"Run headless","text":"<p>Launch the visualizer with a virtual display and export an <code>.mp4</code>:</p> <pre><code>xvfb-run -s \"-screen 0 1280x720x24\" ./visualize\n</code></pre> <p>Adjust the screen size and color depth as needed. The <code>xvfb-run</code> wrapper allows Raylib to render without an attached display, which is convenient for servers and CI jobs.</p>"},{"location":"womd-editor/","title":"WOMD Editor","text":"<p>A browser-based playground for inspecting and editing Waymo Open Motion Dataset (WOMD) scenes. The tool runs fully client-side at https://womd-editor.vercel.app/ and works directly with the JSON format produced by Waymo/ScenarioMax exports and PufferDrive conversions.</p>"},{"location":"womd-editor/#quick-start","title":"Quick start","text":"<ul> <li>Open https://womd-editor.vercel.app/ in a modern Chromium/Firefox browser.</li> <li>Click Import JSON\u2026 in the left sidebar and drop one or more scenario files (Waymo/ScenarioMax JSON or editor exports).</li> <li>The app stores everything in-memory only; nothing is uploaded to a server.</li> </ul>"},{"location":"womd-editor/#what-you-can-do","title":"What you can do","text":"<ul> <li>Inspect: Top-down canvas with zoom/pan/rotate, agent labels, and a playback timeline with variable speed.</li> <li>Edit trajectories: Select an agent and tweak paths via drag handles, draw a polyline with the Line tool, freehand record a path, or drive the agent with keyboard controls (WASD/arrow keys, Space to brake, Enter to save, Esc to cancel).</li> <li>Edit roads: Switch to Road mode to draw or refine lane/edge/crosswalk geometry, recolor vertices by elevation, and view the lane connectivity overlay when ROAD_LANE/ROAD_LINE data exists.</li> <li>Configure metadata: Rename the scenario, toggle label mode (ID vs. array index), mark agents as experts, and choose which agents belong to <code>tracks_to_predict</code>.</li> <li>Export: Preview changes versus the import baseline, then download either Waymo-style JSON or a compact <code>.bin</code> suitable for PufferDrive\u2019s loader.</li> </ul>"},{"location":"womd-editor/#editing-workflow","title":"Editing workflow","text":"<ol> <li>Load a scene: Import one or multiple JSONs; each appears as a row in the Scenarios list with a quick delete button.</li> <li>Playback: Use the timeline to scrub frames or Space/Arrow keys to play/pause/step. Agent labels and trajectory visibility can be toggled in the editor panel.</li> <li>Trajectory tools (Trajectory mode):</li> <li>Adjust Path: Drag existing vertices/handles on the canvas.</li> <li>Line Tool: Click to lay out a polyline, set per-segment duration (seconds), then Apply Path to rebuild timestamps/velocity.</li> <li>Record Path: Freehand capture a path with the pointer; playback resets to frame 0.</li> <li>Drive Agent: Enter a lightweight driving loop; W/A/S/D or arrow keys steer, Space brakes, Enter saves, Esc cancels. Tunable speed/accel/steer sliders live under \u201cDrive Tune.\u201d</li> <li>Road tools (Road mode):</li> <li>Edit Geometry: Select segments/vertices to move, insert, split, or delete (Shift/Ctrl-click to insert on-canvas; Alt/Cmd-click to delete).</li> <li>Draw Road: Click to add vertices; Enter finishes, Esc cancels. Set the default Z used for new vertices in the right-hand panel.</li> <li>Type &amp; overlays: Tag segments as ROAD_LANE / ROAD_EDGE / ROAD_LINE / CROSSWALK / OTHER. Enable Color by Z to visualize elevation and Lane Graph to see lane entry/exit nodes plus downstream arrows.</li> <li>Export &amp; diff: Hit Export to open a preview modal that summarizes changes (metadata, agents, roads, tracks_to_predict, bounds, frames). Download JSON for round-tripping or <code>.bin</code> for simulator ingestion.</li> </ol>"},{"location":"womd-editor/#using-exports-with-pufferdrive","title":"Using exports with PufferDrive","text":"<ul> <li>JSON exports retain the Waymo layout (<code>objects</code>, <code>roads</code>, <code>tracks_to_predict</code>, <code>tl_states</code>, <code>metadata</code>) and can be converted or re-imported.</li> <li><code>.bin</code> exports match the compact format read by <code>pufferlib/ocean/drive/drive.py</code>; drop them into <code>resources/drive/binaries</code> (e.g., <code>map_000.bin</code>) to test inside the simulator.</li> <li>The editor auto-fills missing headings/speeds and clamps degenerate lanes to keep bounds reasonable; always spot-check via the Export preview before committing.</li> </ul>"},{"location":"womd-editor/#notes","title":"Notes","text":"<ul> <li>The app is currently work-in-progress; there is no persistent storage or backend sync.</li> <li>Large scenes may render slowly on low-power GPUs\u2014hide trajectories or road overlays to keep the canvas responsive.</li> <li>Source lives in the <code>WOMD-Editor/web</code> directory of this repo if you want to run it locally with <code>npm install &amp;&amp; npm run dev</code>.</li> </ul>"}]}